{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "binary sentiment classifier using a feed-forward neural network.ipynb",
      "provenance": [],
      "mount_file_id": "1BtEMPPkznMCC_RZ5poncyNAeqR-NEfep",
      "authorship_tag": "ABX9TyNv/ftg/GUE7fYYU9NZqkc7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cleopatra27/feed_forward_neural-network_binary_classifier/blob/main/binary_sentiment_classifier_using_a_feed_forward_neural%C2%A0network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gwZrmYqLUf-y"
      },
      "source": [
        "**Building a binary sentiment classifier using a feed-forward neural network.**\n",
        "\n",
        "We will Treat only word unigrams as features for your neural network classifier, our neural network will have an input layer, 2 hidden layers, and an output layer, train our classifier, then use 10-fold cross validation to optimize parameters using “accuracy” as the metric. For example, you can choose the activation function and choose the number of nodes in the first hidden layer. \n",
        "\n",
        "We will then use the parameters from best performing model and train this neural network on our whole training corpus again. \n",
        "\n",
        "Then we will classify each review in the test set as either positive of negative using your best performing classifier "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZp4i9jxUvTh"
      },
      "source": [
        "import tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X1JKLH2ooCM_",
        "outputId": "a33db43d-5da1-46e5-93f5-ac5abe55f3ac"
      },
      "source": [
        "import tensorflow as tf\n",
        "print(\"TensorFlow version:\", tf.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow version: 2.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9SfqZR3nU01-"
      },
      "source": [
        "Load training data from my google drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1yod5bXCpy0M"
      },
      "source": [
        "import glob\n",
        "files = glob.glob(\"/content/drive/MyDrive/imdb/train/*\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZFV8487U7CV"
      },
      "source": [
        "I loaded the data into a data frame and set our sentiments to 1 for positive and 0 for negative"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "d7o7vD812aPs",
        "outputId": "46db0266-6461-42d3-b21f-e32797fbcd8e"
      },
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "rows = []\n",
        "for folder in files:\n",
        "  for file in glob.glob(folder+'/*'):\n",
        "        with open(os.path.join(os.getcwd(), file), 'r', encoding=\"utf-8-sig\", errors='ignore') as suffix:\n",
        "            sentence = suffix.read().split('\\n')\n",
        "            for line in sentence:\n",
        "                targ = 0\n",
        "                if \"pos\" in file:\n",
        "                  targ = 1\n",
        "                rows.append([line, targ])\n",
        "\n",
        "df = pd.DataFrame(rows, columns=[\"text\", \"sentiment\"])\n",
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>tristar / 1 : 30 / 1997 / r ( language , viole...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td></td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>the brady bunch movie is less a motion picture...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td></td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>i'm going to keep this plot summary brief , so...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2261</th>\n",
              "      <td></td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2262</th>\n",
              "      <td>what starts out as a monotonous talking-head m...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2263</th>\n",
              "      <td></td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2264</th>\n",
              "      <td>jackie brown ( miramax - 1997 ) starring pam g...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2265</th>\n",
              "      <td></td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2266 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   text  sentiment\n",
              "0     tristar / 1 : 30 / 1997 / r ( language , viole...          0\n",
              "1                                                                0\n",
              "2     the brady bunch movie is less a motion picture...          0\n",
              "3                                                                0\n",
              "4     i'm going to keep this plot summary brief , so...          0\n",
              "...                                                 ...        ...\n",
              "2261                                                             1\n",
              "2262  what starts out as a monotonous talking-head m...          1\n",
              "2263                                                             1\n",
              "2264  jackie brown ( miramax - 1997 ) starring pam g...          1\n",
              "2265                                                             1\n",
              "\n",
              "[2266 rows x 2 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gvETiESTVPlu"
      },
      "source": [
        "Preview of the total positive and negative reviews we have in our data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yEWUdLc2CtJ4",
        "outputId": "714bf142-e5aa-45af-d6f0-b26933aea413"
      },
      "source": [
        "print((df.sentiment == 1). sum()) # positive\n",
        "print((df.sentiment == 0). sum()) # negative"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1132\n",
            "1134\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2B2OEDNVYfU"
      },
      "source": [
        "Next i cleaned up the data, starting with punctuations and URLS.\n",
        "Here, i defined a function to handle punctuations and another for URLS."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "M-qUTKWu38KF",
        "outputId": "a0434156-6756-4f1e-ceb1-66afb8b08ae4"
      },
      "source": [
        "import re\n",
        "import string \n",
        "\n",
        "def remove_URL(text):\n",
        "  url = re.compile(r\"https?:\\/\\/.*[\\r\\n]*\")\n",
        "  return url.sub(r\"\", text)\n",
        "\n",
        "def remove_punctuation(text):\n",
        "  translator = str.maketrans(\"\", \"\", string.punctuation)\n",
        "  return text.translate(translator)\n",
        "\n",
        "string.punctuation"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kJfahkkW489Q"
      },
      "source": [
        "pattern = re.compile(r\"https?:\\/\\/.*[\\r\\n]*\")\n",
        "for t in df.text:\n",
        "  matches = pattern.findall(t)\n",
        "  for match in matches:\n",
        "    print(t)\n",
        "    print(match)\n",
        "    print(pattern.sub(r\"\", t))\n",
        "  if len(matches) > 0:\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijrXBY8MVrcD"
      },
      "source": [
        "Cleaned the text by calling the punctuation and URL removal function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KgbDT7At5arf"
      },
      "source": [
        "df[\"text\"] = df.text.map(remove_URL)\n",
        "df[\"text\"] = df.text.map(remove_punctuation)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLILoTy6V2Yu"
      },
      "source": [
        "Implementted function to remove stop words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "30XiNHPD5kj9",
        "outputId": "c82e0d05-7e4c-4470-a2c2-62a9e47558a5"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "stop = set(stopwords.words(\"english\"))\n",
        "\n",
        "def remove_stopwords(text):\n",
        "  filtered_words = [word.lower() for word in text.split() if word.lower() not in stop]\n",
        "  return \" \".join(filtered_words)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_yJ5eW7qV60E"
      },
      "source": [
        "Preview of the stop words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QSQiyZ326EnX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc69ceeb-b22e-49ba-b16e-09430a3a97dd"
      },
      "source": [
        "stop"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'a',\n",
              " 'about',\n",
              " 'above',\n",
              " 'after',\n",
              " 'again',\n",
              " 'against',\n",
              " 'ain',\n",
              " 'all',\n",
              " 'am',\n",
              " 'an',\n",
              " 'and',\n",
              " 'any',\n",
              " 'are',\n",
              " 'aren',\n",
              " \"aren't\",\n",
              " 'as',\n",
              " 'at',\n",
              " 'be',\n",
              " 'because',\n",
              " 'been',\n",
              " 'before',\n",
              " 'being',\n",
              " 'below',\n",
              " 'between',\n",
              " 'both',\n",
              " 'but',\n",
              " 'by',\n",
              " 'can',\n",
              " 'couldn',\n",
              " \"couldn't\",\n",
              " 'd',\n",
              " 'did',\n",
              " 'didn',\n",
              " \"didn't\",\n",
              " 'do',\n",
              " 'does',\n",
              " 'doesn',\n",
              " \"doesn't\",\n",
              " 'doing',\n",
              " 'don',\n",
              " \"don't\",\n",
              " 'down',\n",
              " 'during',\n",
              " 'each',\n",
              " 'few',\n",
              " 'for',\n",
              " 'from',\n",
              " 'further',\n",
              " 'had',\n",
              " 'hadn',\n",
              " \"hadn't\",\n",
              " 'has',\n",
              " 'hasn',\n",
              " \"hasn't\",\n",
              " 'have',\n",
              " 'haven',\n",
              " \"haven't\",\n",
              " 'having',\n",
              " 'he',\n",
              " 'her',\n",
              " 'here',\n",
              " 'hers',\n",
              " 'herself',\n",
              " 'him',\n",
              " 'himself',\n",
              " 'his',\n",
              " 'how',\n",
              " 'i',\n",
              " 'if',\n",
              " 'in',\n",
              " 'into',\n",
              " 'is',\n",
              " 'isn',\n",
              " \"isn't\",\n",
              " 'it',\n",
              " \"it's\",\n",
              " 'its',\n",
              " 'itself',\n",
              " 'just',\n",
              " 'll',\n",
              " 'm',\n",
              " 'ma',\n",
              " 'me',\n",
              " 'mightn',\n",
              " \"mightn't\",\n",
              " 'more',\n",
              " 'most',\n",
              " 'mustn',\n",
              " \"mustn't\",\n",
              " 'my',\n",
              " 'myself',\n",
              " 'needn',\n",
              " \"needn't\",\n",
              " 'no',\n",
              " 'nor',\n",
              " 'not',\n",
              " 'now',\n",
              " 'o',\n",
              " 'of',\n",
              " 'off',\n",
              " 'on',\n",
              " 'once',\n",
              " 'only',\n",
              " 'or',\n",
              " 'other',\n",
              " 'our',\n",
              " 'ours',\n",
              " 'ourselves',\n",
              " 'out',\n",
              " 'over',\n",
              " 'own',\n",
              " 're',\n",
              " 's',\n",
              " 'same',\n",
              " 'shan',\n",
              " \"shan't\",\n",
              " 'she',\n",
              " \"she's\",\n",
              " 'should',\n",
              " \"should've\",\n",
              " 'shouldn',\n",
              " \"shouldn't\",\n",
              " 'so',\n",
              " 'some',\n",
              " 'such',\n",
              " 't',\n",
              " 'than',\n",
              " 'that',\n",
              " \"that'll\",\n",
              " 'the',\n",
              " 'their',\n",
              " 'theirs',\n",
              " 'them',\n",
              " 'themselves',\n",
              " 'then',\n",
              " 'there',\n",
              " 'these',\n",
              " 'they',\n",
              " 'this',\n",
              " 'those',\n",
              " 'through',\n",
              " 'to',\n",
              " 'too',\n",
              " 'under',\n",
              " 'until',\n",
              " 'up',\n",
              " 've',\n",
              " 'very',\n",
              " 'was',\n",
              " 'wasn',\n",
              " \"wasn't\",\n",
              " 'we',\n",
              " 'were',\n",
              " 'weren',\n",
              " \"weren't\",\n",
              " 'what',\n",
              " 'when',\n",
              " 'where',\n",
              " 'which',\n",
              " 'while',\n",
              " 'who',\n",
              " 'whom',\n",
              " 'why',\n",
              " 'will',\n",
              " 'with',\n",
              " 'won',\n",
              " \"won't\",\n",
              " 'wouldn',\n",
              " \"wouldn't\",\n",
              " 'y',\n",
              " 'you',\n",
              " \"you'd\",\n",
              " \"you'll\",\n",
              " \"you're\",\n",
              " \"you've\",\n",
              " 'your',\n",
              " 'yours',\n",
              " 'yourself',\n",
              " 'yourselves'}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMWRzq55WGkh"
      },
      "source": [
        "Removed stopwords"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3cfl4B46ICV"
      },
      "source": [
        "df[\"text\"] = df.text.map(remove_stopwords)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Nc0vXVbWKu4"
      },
      "source": [
        "Now the shape of our dataframe is"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fSx-SRbi6NSy",
        "outputId": "eb421d9d-9f39-4bb3-bf1c-af182a8760a0"
      },
      "source": [
        "df.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2266, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OClrfIUEW5r6"
      },
      "source": [
        "Preview of our text column"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gp6HzRvIW33d",
        "outputId": "a3ed0e3c-ec99-47fe-cd5e-8149da2ffd91"
      },
      "source": [
        "df.text"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       tristar 1 30 1997 r language violence dennis r...\n",
              "1                                                        \n",
              "2       brady bunch movie less motion picture minor po...\n",
              "3                                                        \n",
              "4       im going keep plot summary brief something wis...\n",
              "                              ...                        \n",
              "2261                                                     \n",
              "2262    starts monotonous talkinghead musical history ...\n",
              "2263                                                     \n",
              "2264    jackie brown miramax 1997 starring pam grier s...\n",
              "2265                                                     \n",
              "Name: text, Length: 2266, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4AdwEaCW8_Z"
      },
      "source": [
        "I prepared the data to a format our model can take, by tokenizing our words."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jtuE3uDbXRdP"
      },
      "source": [
        "We start by getting our unique words and implementing a word frequency counter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJSXy8lU6OHK"
      },
      "source": [
        "from collections import Counter\n",
        "\n",
        "def counter_word(text_col):\n",
        "  count = Counter()\n",
        "  for text in text_col.values:\n",
        "    for word in text.split():\n",
        "      count[word] += 1\n",
        "  return count\n",
        "\n",
        "counter = counter_word(df.text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1o4wv8eeXeTl"
      },
      "source": [
        "The most common words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Akml32-V6yoZ",
        "outputId": "84617395-a4be-4cba-88c3-fae5033182cd"
      },
      "source": [
        "counter.most_common(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('film', 5034),\n",
              " ('movie', 3249),\n",
              " ('one', 3012),\n",
              " ('like', 2036),\n",
              " ('even', 1421)]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMQLpnavXjGq"
      },
      "source": [
        "Our number of unique words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E8dZB84j6243"
      },
      "source": [
        "num_unique_words = len(counter)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qzIFUmvXrXn"
      },
      "source": [
        "Tokenizing the texts "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1FszV02-nHo"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "tokenizer = Tokenizer(num_words=num_unique_words)\n",
        "tokenizer.fit_on_texts(df.text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qAc5tfK7YV0h"
      },
      "source": [
        "Preview the word index"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IH7LDFLy_MjY"
      },
      "source": [
        "word_index = tokenizer.word_index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "izgE5pj4_qiI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30912de4-3d45-4d3b-94f6-7f61744f2175"
      },
      "source": [
        "word_index"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'film': 1,\n",
              " 'movie': 2,\n",
              " 'one': 3,\n",
              " 'like': 4,\n",
              " 'even': 5,\n",
              " 'time': 6,\n",
              " 'good': 7,\n",
              " 'films': 8,\n",
              " 'also': 9,\n",
              " 'story': 10,\n",
              " 'much': 11,\n",
              " 'get': 12,\n",
              " 'characters': 13,\n",
              " 'would': 14,\n",
              " 'two': 15,\n",
              " 'first': 16,\n",
              " 'character': 17,\n",
              " 'see': 18,\n",
              " 'well': 19,\n",
              " 'way': 20,\n",
              " 'really': 21,\n",
              " 'make': 22,\n",
              " 'little': 23,\n",
              " 'people': 24,\n",
              " 'plot': 25,\n",
              " 'movies': 26,\n",
              " 'never': 27,\n",
              " 'life': 28,\n",
              " 'could': 29,\n",
              " 'bad': 30,\n",
              " 'scene': 31,\n",
              " 'new': 32,\n",
              " 'man': 33,\n",
              " 'know': 34,\n",
              " 'director': 35,\n",
              " 'many': 36,\n",
              " 'dont': 37,\n",
              " 'hes': 38,\n",
              " 'best': 39,\n",
              " 'great': 40,\n",
              " 'scenes': 41,\n",
              " 'doesnt': 42,\n",
              " 'another': 43,\n",
              " 'us': 44,\n",
              " 'action': 45,\n",
              " 'love': 46,\n",
              " 'something': 47,\n",
              " 'made': 48,\n",
              " 'theres': 49,\n",
              " 'minutes': 50,\n",
              " 'still': 51,\n",
              " 'back': 52,\n",
              " 'john': 53,\n",
              " 'cast': 54,\n",
              " 'go': 55,\n",
              " 'makes': 56,\n",
              " 'end': 57,\n",
              " 'years': 58,\n",
              " 'seems': 59,\n",
              " 'however': 60,\n",
              " 'work': 61,\n",
              " 'things': 62,\n",
              " 'every': 63,\n",
              " 'since': 64,\n",
              " 'actually': 65,\n",
              " 'gets': 66,\n",
              " 'going': 67,\n",
              " 'big': 68,\n",
              " 'around': 69,\n",
              " 'better': 70,\n",
              " 'role': 71,\n",
              " 'think': 72,\n",
              " 'may': 73,\n",
              " 'seen': 74,\n",
              " 'last': 75,\n",
              " 'real': 76,\n",
              " 'audience': 77,\n",
              " 'enough': 78,\n",
              " 'although': 79,\n",
              " 'though': 80,\n",
              " 'performance': 81,\n",
              " 'isnt': 82,\n",
              " 'thats': 83,\n",
              " 'world': 84,\n",
              " 'take': 85,\n",
              " 'fact': 86,\n",
              " 'http': 87,\n",
              " 'long': 88,\n",
              " 'funny': 89,\n",
              " 'comedy': 90,\n",
              " 'comes': 91,\n",
              " 'right': 92,\n",
              " 'thing': 93,\n",
              " 'look': 94,\n",
              " 'find': 95,\n",
              " 'say': 96,\n",
              " 'almost': 97,\n",
              " 'come': 98,\n",
              " 'played': 99,\n",
              " 'directed': 100,\n",
              " 'plays': 101,\n",
              " 'nothing': 102,\n",
              " 'young': 103,\n",
              " 'ever': 104,\n",
              " 'old': 105,\n",
              " 'original': 106,\n",
              " 'lot': 107,\n",
              " 'actors': 108,\n",
              " 'im': 109,\n",
              " 'script': 110,\n",
              " 'course': 111,\n",
              " 'three': 112,\n",
              " 'without': 113,\n",
              " 'cant': 114,\n",
              " 'takes': 115,\n",
              " 'part': 116,\n",
              " 'written': 117,\n",
              " 'least': 118,\n",
              " 'show': 119,\n",
              " 'anything': 120,\n",
              " 'acting': 121,\n",
              " 'picture': 122,\n",
              " 'family': 123,\n",
              " 'michael': 124,\n",
              " 'year': 125,\n",
              " 'interesting': 126,\n",
              " 'pretty': 127,\n",
              " 'screen': 128,\n",
              " 'david': 129,\n",
              " 'away': 130,\n",
              " 'point': 131,\n",
              " 'quite': 132,\n",
              " 'rather': 133,\n",
              " 'goes': 134,\n",
              " 'might': 135,\n",
              " 'www': 136,\n",
              " 'far': 137,\n",
              " 'star': 138,\n",
              " 'always': 139,\n",
              " 'fun': 140,\n",
              " 'must': 141,\n",
              " 'effects': 142,\n",
              " 'times': 143,\n",
              " 'instead': 144,\n",
              " 'kind': 145,\n",
              " 'seem': 146,\n",
              " 'day': 147,\n",
              " 'yet': 148,\n",
              " 'set': 149,\n",
              " 'watch': 150,\n",
              " 'place': 151,\n",
              " 'sense': 152,\n",
              " 'guy': 153,\n",
              " 'american': 154,\n",
              " 'starring': 155,\n",
              " 'woman': 156,\n",
              " 'everything': 157,\n",
              " 'job': 158,\n",
              " 'home': 159,\n",
              " 'series': 160,\n",
              " 'probably': 161,\n",
              " 'want': 162,\n",
              " 'didnt': 163,\n",
              " 'bit': 164,\n",
              " 'com': 165,\n",
              " 'give': 166,\n",
              " 'based': 167,\n",
              " 'james': 168,\n",
              " 'music': 169,\n",
              " 'help': 170,\n",
              " 'stars': 171,\n",
              " 'wife': 172,\n",
              " 'trying': 173,\n",
              " 'making': 174,\n",
              " 'sure': 175,\n",
              " 'running': 176,\n",
              " 'given': 177,\n",
              " 'screenplay': 178,\n",
              " 'special': 179,\n",
              " 'whole': 180,\n",
              " 'black': 181,\n",
              " 'gives': 182,\n",
              " 'review': 183,\n",
              " 'watching': 184,\n",
              " 'actor': 185,\n",
              " 'along': 186,\n",
              " 'violence': 187,\n",
              " 'together': 188,\n",
              " 'sex': 189,\n",
              " 'hard': 190,\n",
              " 'play': 191,\n",
              " 'moments': 192,\n",
              " 'men': 193,\n",
              " 'case': 194,\n",
              " 'reviews': 195,\n",
              " 'dialogue': 196,\n",
              " 'everyone': 197,\n",
              " 'next': 198,\n",
              " 'r': 199,\n",
              " 'city': 200,\n",
              " 'hollywood': 201,\n",
              " 'perhaps': 202,\n",
              " 'money': 203,\n",
              " 'become': 204,\n",
              " 'less': 205,\n",
              " 'becomes': 206,\n",
              " 'wants': 207,\n",
              " 'feel': 208,\n",
              " 'different': 209,\n",
              " 'got': 210,\n",
              " 'half': 211,\n",
              " 'rated': 212,\n",
              " '2': 213,\n",
              " 'either': 214,\n",
              " 'found': 215,\n",
              " 'looking': 216,\n",
              " 'ending': 217,\n",
              " 'simply': 218,\n",
              " 'soon': 219,\n",
              " 'robert': 220,\n",
              " 'use': 221,\n",
              " 'done': 222,\n",
              " 'especially': 223,\n",
              " 'high': 224,\n",
              " 'shes': 225,\n",
              " 'put': 226,\n",
              " 'theyre': 227,\n",
              " 'entire': 228,\n",
              " 'death': 229,\n",
              " 'keep': 230,\n",
              " 'human': 231,\n",
              " 'night': 232,\n",
              " 'anyone': 233,\n",
              " 'small': 234,\n",
              " 'turns': 235,\n",
              " 'shows': 236,\n",
              " 'left': 237,\n",
              " 'whose': 238,\n",
              " 'looks': 239,\n",
              " 'school': 240,\n",
              " 'performances': 241,\n",
              " 'rest': 242,\n",
              " 'tries': 243,\n",
              " 'horror': 244,\n",
              " 'father': 245,\n",
              " 'problem': 246,\n",
              " 'friends': 247,\n",
              " 'main': 248,\n",
              " 'several': 249,\n",
              " 'getting': 250,\n",
              " 'kids': 251,\n",
              " 'completely': 252,\n",
              " 'house': 253,\n",
              " 'reason': 254,\n",
              " 'de': 255,\n",
              " 'humor': 256,\n",
              " 'line': 257,\n",
              " 'certainly': 258,\n",
              " 'named': 259,\n",
              " 'lost': 260,\n",
              " 'later': 261,\n",
              " 'girl': 262,\n",
              " 'else': 263,\n",
              " '1': 264,\n",
              " 'war': 265,\n",
              " 'begins': 266,\n",
              " 'past': 267,\n",
              " 'couple': 268,\n",
              " 'playing': 269,\n",
              " 'true': 270,\n",
              " 'finally': 271,\n",
              " 'idea': 272,\n",
              " 'name': 273,\n",
              " 'thought': 274,\n",
              " 'able': 275,\n",
              " 'often': 276,\n",
              " 'tell': 277,\n",
              " 'tom': 278,\n",
              " 'friend': 279,\n",
              " 'nice': 280,\n",
              " 'second': 281,\n",
              " 'said': 282,\n",
              " 'someone': 283,\n",
              " 'kevin': 284,\n",
              " 'mind': 285,\n",
              " 'dead': 286,\n",
              " 'maybe': 287,\n",
              " 'ive': 288,\n",
              " 'behind': 289,\n",
              " 'final': 290,\n",
              " 'sequence': 291,\n",
              " 'book': 292,\n",
              " 'despite': 293,\n",
              " 'group': 294,\n",
              " 'unfortunately': 295,\n",
              " 'others': 296,\n",
              " 'finds': 297,\n",
              " 'run': 298,\n",
              " 'relationship': 299,\n",
              " 'called': 300,\n",
              " 'entertaining': 301,\n",
              " 'game': 302,\n",
              " 'mother': 303,\n",
              " 'evil': 304,\n",
              " 'believe': 305,\n",
              " 'wrong': 306,\n",
              " 'starts': 307,\n",
              " 'seeing': 308,\n",
              " 'days': 309,\n",
              " 'turn': 310,\n",
              " 'person': 311,\n",
              " 'try': 312,\n",
              " 'lives': 313,\n",
              " 'boy': 314,\n",
              " 'start': 315,\n",
              " 'comic': 316,\n",
              " 'works': 317,\n",
              " 'youre': 318,\n",
              " 'example': 319,\n",
              " 'full': 320,\n",
              " 'novel': 321,\n",
              " 'throughout': 322,\n",
              " 'used': 323,\n",
              " 'live': 324,\n",
              " 'peter': 325,\n",
              " 'matter': 326,\n",
              " 'lee': 327,\n",
              " 'town': 328,\n",
              " 'head': 329,\n",
              " 'style': 330,\n",
              " 'guys': 331,\n",
              " 'summer': 332,\n",
              " 'hand': 333,\n",
              " 'worth': 334,\n",
              " 'opening': 335,\n",
              " 'side': 336,\n",
              " 'lines': 337,\n",
              " 'camera': 338,\n",
              " 'production': 339,\n",
              " 'roles': 340,\n",
              " 'title': 341,\n",
              " 'nearly': 342,\n",
              " 'worst': 343,\n",
              " 'shot': 344,\n",
              " 'car': 345,\n",
              " 'problems': 346,\n",
              " 'visit': 347,\n",
              " 'supposed': 348,\n",
              " 'dark': 349,\n",
              " 'white': 350,\n",
              " 'short': 351,\n",
              " 'including': 352,\n",
              " 'tv': 353,\n",
              " 'care': 354,\n",
              " 'children': 355,\n",
              " 'van': 356,\n",
              " 'daughter': 357,\n",
              " 'supporting': 358,\n",
              " 'five': 359,\n",
              " 'joe': 360,\n",
              " 'version': 361,\n",
              " 'fine': 362,\n",
              " 'upon': 363,\n",
              " 'hour': 364,\n",
              " 'scream': 365,\n",
              " 'drama': 366,\n",
              " 'language': 367,\n",
              " 'jack': 368,\n",
              " 'order': 369,\n",
              " 'mr': 370,\n",
              " 'boring': 371,\n",
              " 'perfect': 372,\n",
              " 'direction': 373,\n",
              " 'need': 374,\n",
              " 'face': 375,\n",
              " 'top': 376,\n",
              " 'son': 377,\n",
              " 'already': 378,\n",
              " 'york': 379,\n",
              " 'beginning': 380,\n",
              " 'obvious': 381,\n",
              " 'sometimes': 382,\n",
              " 'team': 383,\n",
              " 'whos': 384,\n",
              " 'arent': 385,\n",
              " 'sequences': 386,\n",
              " 'fight': 387,\n",
              " 'let': 388,\n",
              " 'truly': 389,\n",
              " 'computer': 390,\n",
              " 'beautiful': 391,\n",
              " 'alien': 392,\n",
              " 'says': 393,\n",
              " 'strong': 394,\n",
              " 'early': 395,\n",
              " 'hours': 396,\n",
              " 'wasnt': 397,\n",
              " 'yes': 398,\n",
              " 'paul': 399,\n",
              " 'genre': 400,\n",
              " 'save': 401,\n",
              " 'deal': 402,\n",
              " 'space': 403,\n",
              " 'exactly': 404,\n",
              " 'major': 405,\n",
              " 'video': 406,\n",
              " 'jackie': 407,\n",
              " 'career': 408,\n",
              " 'kill': 409,\n",
              " 'material': 410,\n",
              " 'sort': 411,\n",
              " 'simple': 412,\n",
              " 'eventually': 413,\n",
              " 'deep': 414,\n",
              " 'tells': 415,\n",
              " 'eyes': 416,\n",
              " 'hit': 417,\n",
              " 'late': 418,\n",
              " '1998': 419,\n",
              " 'release': 420,\n",
              " 'chris': 421,\n",
              " 'knows': 422,\n",
              " 'level': 423,\n",
              " 'email': 424,\n",
              " 'runs': 425,\n",
              " 'worse': 426,\n",
              " 'emotional': 427,\n",
              " 'history': 428,\n",
              " 'act': 429,\n",
              " 'earth': 430,\n",
              " 'ends': 431,\n",
              " 'quickly': 432,\n",
              " 'william': 433,\n",
              " 'brothers': 434,\n",
              " 'talent': 435,\n",
              " 'wont': 436,\n",
              " 'moment': 437,\n",
              " 'richard': 438,\n",
              " 'wild': 439,\n",
              " 'child': 440,\n",
              " 'four': 441,\n",
              " 'sets': 442,\n",
              " 'stupid': 443,\n",
              " 'scott': 444,\n",
              " 'mostly': 445,\n",
              " 'bob': 446,\n",
              " 'close': 447,\n",
              " 'known': 448,\n",
              " 'theater': 449,\n",
              " 'none': 450,\n",
              " 'ones': 451,\n",
              " 'classic': 452,\n",
              " 'released': 453,\n",
              " 'voice': 454,\n",
              " 'brother': 455,\n",
              " 'martin': 456,\n",
              " 'whats': 457,\n",
              " 'number': 458,\n",
              " 'feature': 459,\n",
              " 'body': 460,\n",
              " 'attempt': 461,\n",
              " 'taken': 462,\n",
              " 'particularly': 463,\n",
              " 'sexual': 464,\n",
              " 'experience': 465,\n",
              " 'enjoy': 466,\n",
              " 'fall': 467,\n",
              " 'coming': 468,\n",
              " 'rating': 469,\n",
              " 'credits': 470,\n",
              " 'happens': 471,\n",
              " 'sound': 472,\n",
              " 'murder': 473,\n",
              " 'obviously': 474,\n",
              " 'planet': 475,\n",
              " 'involved': 476,\n",
              " 'women': 477,\n",
              " 'attention': 478,\n",
              " 'appears': 479,\n",
              " 'stop': 480,\n",
              " 'police': 481,\n",
              " 'extremely': 482,\n",
              " 'youll': 483,\n",
              " 'parents': 484,\n",
              " 'bill': 485,\n",
              " 'usually': 486,\n",
              " 'result': 487,\n",
              " 'piece': 488,\n",
              " 'members': 489,\n",
              " 'brings': 490,\n",
              " 'working': 491,\n",
              " 'produced': 492,\n",
              " 'thriller': 493,\n",
              " 'george': 494,\n",
              " 'across': 495,\n",
              " 'certain': 496,\n",
              " 'lead': 497,\n",
              " 'falls': 498,\n",
              " 'die': 499,\n",
              " 'learn': 500,\n",
              " 'forced': 501,\n",
              " 'steve': 502,\n",
              " 'room': 503,\n",
              " 'mean': 504,\n",
              " 'wonder': 505,\n",
              " 'words': 506,\n",
              " 'saw': 507,\n",
              " 'interest': 508,\n",
              " 'ship': 509,\n",
              " 'happen': 510,\n",
              " 'leave': 511,\n",
              " 'recent': 512,\n",
              " 'question': 513,\n",
              " 'battle': 514,\n",
              " 'wonderful': 515,\n",
              " 'killed': 516,\n",
              " 'serious': 517,\n",
              " 'elements': 518,\n",
              " 'talk': 519,\n",
              " 'whether': 520,\n",
              " 'crime': 521,\n",
              " 'lack': 522,\n",
              " 'motion': 523,\n",
              " 'max': 524,\n",
              " 'killer': 525,\n",
              " 'hope': 526,\n",
              " 'form': 527,\n",
              " 'jokes': 528,\n",
              " 'within': 529,\n",
              " 'involving': 530,\n",
              " 'somewhat': 531,\n",
              " 'remember': 532,\n",
              " 'attempts': 533,\n",
              " 'read': 534,\n",
              " 'alone': 535,\n",
              " 'reviewed': 536,\n",
              " 'note': 537,\n",
              " 'premise': 538,\n",
              " 'oscar': 539,\n",
              " 'laughs': 540,\n",
              " 'cool': 541,\n",
              " 'smith': 542,\n",
              " 'king': 543,\n",
              " 'writer': 544,\n",
              " 'taking': 545,\n",
              " 'meet': 546,\n",
              " 'somehow': 547,\n",
              " 'bring': 548,\n",
              " 'important': 549,\n",
              " 'talking': 550,\n",
              " 'viewers': 551,\n",
              " 'possible': 552,\n",
              " 'art': 553,\n",
              " 'power': 554,\n",
              " 'brief': 555,\n",
              " 'dr': 556,\n",
              " 'manages': 557,\n",
              " 'features': 558,\n",
              " 'easily': 559,\n",
              " 'happy': 560,\n",
              " '1999': 561,\n",
              " 'oh': 562,\n",
              " 'surprise': 563,\n",
              " 'among': 564,\n",
              " 'meets': 565,\n",
              " 'audiences': 566,\n",
              " 'fiction': 567,\n",
              " 'mark': 568,\n",
              " 'age': 569,\n",
              " 'call': 570,\n",
              " 'smart': 571,\n",
              " 'hell': 572,\n",
              " 'complete': 573,\n",
              " 'except': 574,\n",
              " 'laugh': 575,\n",
              " 'guess': 576,\n",
              " 'hilarious': 577,\n",
              " 'using': 578,\n",
              " 'secret': 579,\n",
              " 'free': 580,\n",
              " 'large': 581,\n",
              " 'robin': 582,\n",
              " 'subject': 583,\n",
              " 'local': 584,\n",
              " 'filmmakers': 585,\n",
              " 'ill': 586,\n",
              " 'entertainment': 587,\n",
              " 'html': 588,\n",
              " 'television': 589,\n",
              " 'feels': 590,\n",
              " 'ago': 591,\n",
              " 'events': 592,\n",
              " 'business': 593,\n",
              " 'ben': 594,\n",
              " 'begin': 595,\n",
              " 'chance': 596,\n",
              " 'science': 597,\n",
              " 'word': 598,\n",
              " 'agent': 599,\n",
              " 'leads': 600,\n",
              " 'crew': 601,\n",
              " 'surprisingly': 602,\n",
              " 'prison': 603,\n",
              " 'youve': 604,\n",
              " 'difficult': 605,\n",
              " 'dog': 606,\n",
              " 'poor': 607,\n",
              " 'office': 608,\n",
              " 'tale': 609,\n",
              " 'sequel': 610,\n",
              " 'personal': 611,\n",
              " 'giving': 612,\n",
              " 'former': 613,\n",
              " 'third': 614,\n",
              " 'heart': 615,\n",
              " 'uses': 616,\n",
              " 'visual': 617,\n",
              " 'romantic': 618,\n",
              " 'score': 619,\n",
              " 'predictable': 620,\n",
              " 'couldnt': 621,\n",
              " 'sam': 622,\n",
              " 'mission': 623,\n",
              " 'harry': 624,\n",
              " 'expect': 625,\n",
              " 'romance': 626,\n",
              " 'private': 627,\n",
              " 'similar': 628,\n",
              " 'change': 629,\n",
              " 'fans': 630,\n",
              " 'flick': 631,\n",
              " 'dramatic': 632,\n",
              " 'u': 633,\n",
              " 'stuff': 634,\n",
              " 'beyond': 635,\n",
              " 'williams': 636,\n",
              " 'slow': 637,\n",
              " 'bond': 638,\n",
              " 'disney': 639,\n",
              " 'country': 640,\n",
              " 'message': 641,\n",
              " 'ryan': 642,\n",
              " 'pictures': 643,\n",
              " 'successful': 644,\n",
              " 'mystery': 645,\n",
              " 'living': 646,\n",
              " 'needs': 647,\n",
              " 'future': 648,\n",
              " 'etc': 649,\n",
              " 'actress': 650,\n",
              " 'situations': 651,\n",
              " 'parts': 652,\n",
              " 'park': 653,\n",
              " 'cut': 654,\n",
              " 'following': 655,\n",
              " 'net': 656,\n",
              " 'aspect': 657,\n",
              " 'straight': 658,\n",
              " 'girlfriend': 659,\n",
              " 'mike': 660,\n",
              " 'type': 661,\n",
              " 'went': 662,\n",
              " 'present': 663,\n",
              " 'single': 664,\n",
              " 'water': 665,\n",
              " 'opens': 666,\n",
              " 'mary': 667,\n",
              " 'hero': 668,\n",
              " 'society': 669,\n",
              " 'easy': 670,\n",
              " 'return': 671,\n",
              " 'viewer': 672,\n",
              " 'rich': 673,\n",
              " 'annoying': 674,\n",
              " 'definitely': 675,\n",
              " 'wouldnt': 676,\n",
              " 'husband': 677,\n",
              " 'cop': 678,\n",
              " 'blood': 679,\n",
              " 'lets': 680,\n",
              " 'excellent': 681,\n",
              " 'america': 682,\n",
              " 'turned': 683,\n",
              " 'intelligent': 684,\n",
              " '4': 685,\n",
              " 'clear': 686,\n",
              " 'giant': 687,\n",
              " 'situation': 688,\n",
              " 'huge': 689,\n",
              " 'basically': 690,\n",
              " 'tim': 691,\n",
              " 'critics': 692,\n",
              " 'kid': 693,\n",
              " 'clever': 694,\n",
              " 'feeling': 695,\n",
              " 'christopher': 696,\n",
              " 'strange': 697,\n",
              " 'decides': 698,\n",
              " 'ways': 699,\n",
              " 'towards': 700,\n",
              " 'latest': 701,\n",
              " 'impressive': 702,\n",
              " 'government': 703,\n",
              " 'middle': 704,\n",
              " 'familiar': 705,\n",
              " 'control': 706,\n",
              " 'usual': 707,\n",
              " '2000': 708,\n",
              " 'contains': 709,\n",
              " 'absolutely': 710,\n",
              " 'figure': 711,\n",
              " 'wanted': 712,\n",
              " 'quality': 713,\n",
              " 'came': 714,\n",
              " 'murphy': 715,\n",
              " 'j': 716,\n",
              " 'open': 717,\n",
              " 'potential': 718,\n",
              " 'thinking': 719,\n",
              " 'red': 720,\n",
              " 'law': 721,\n",
              " 'popular': 722,\n",
              " 'girls': 723,\n",
              " 'danny': 724,\n",
              " 'entirely': 725,\n",
              " 'matthew': 726,\n",
              " 'scary': 727,\n",
              " 'told': 728,\n",
              " '10': 729,\n",
              " 'solid': 730,\n",
              " 'near': 731,\n",
              " 'likely': 732,\n",
              " 'gone': 733,\n",
              " 'id': 734,\n",
              " 'writing': 735,\n",
              " 'success': 736,\n",
              " 'immediately': 737,\n",
              " 'drug': 738,\n",
              " 'ultimately': 739,\n",
              " 'powerful': 740,\n",
              " 'tony': 741,\n",
              " 'leaves': 742,\n",
              " 'means': 743,\n",
              " 'saying': 744,\n",
              " 'rock': 745,\n",
              " 'frank': 746,\n",
              " 'apparently': 747,\n",
              " 'sister': 748,\n",
              " 'jones': 749,\n",
              " 'add': 750,\n",
              " 'seemed': 751,\n",
              " 'reality': 752,\n",
              " 'stories': 753,\n",
              " 'woody': 754,\n",
              " 'escape': 755,\n",
              " 'general': 756,\n",
              " 'project': 757,\n",
              " 'member': 758,\n",
              " 'questions': 759,\n",
              " 'earlier': 760,\n",
              " 'dull': 761,\n",
              " 'effective': 762,\n",
              " 'bruce': 763,\n",
              " 'aliens': 764,\n",
              " 'catch': 765,\n",
              " 'created': 766,\n",
              " 'pg13': 767,\n",
              " 'l': 768,\n",
              " 'amusing': 769,\n",
              " 'overall': 770,\n",
              " 'mars': 771,\n",
              " 'inside': 772,\n",
              " 'cannot': 773,\n",
              " 'jerry': 774,\n",
              " 'presence': 775,\n",
              " 'jim': 776,\n",
              " 'suspense': 777,\n",
              " 'dream': 778,\n",
              " 'date': 779,\n",
              " 'allen': 780,\n",
              " 'god': 781,\n",
              " 'batman': 782,\n",
              " 'various': 783,\n",
              " 'break': 784,\n",
              " 'totally': 785,\n",
              " 'nudity': 786,\n",
              " 'wedding': 787,\n",
              " 'stone': 788,\n",
              " 'amazing': 789,\n",
              " '3': 790,\n",
              " 'liked': 791,\n",
              " 'appear': 792,\n",
              " 'plenty': 793,\n",
              " 'slightly': 794,\n",
              " 'jr': 795,\n",
              " 'producers': 796,\n",
              " 'english': 797,\n",
              " 'married': 798,\n",
              " 'effect': 799,\n",
              " 'cinematography': 800,\n",
              " 'moving': 801,\n",
              " 'clearly': 802,\n",
              " 'bunch': 803,\n",
              " 'create': 804,\n",
              " 'otherwise': 805,\n",
              " 'hear': 806,\n",
              " 'shots': 807,\n",
              " 'effort': 808,\n",
              " 'edward': 809,\n",
              " 'copyright': 810,\n",
              " 'patrick': 811,\n",
              " 'nature': 812,\n",
              " 'violent': 813,\n",
              " 'stephen': 814,\n",
              " 'due': 815,\n",
              " 'keeps': 816,\n",
              " 'stay': 817,\n",
              " 'six': 818,\n",
              " 'impossible': 819,\n",
              " 'box': 820,\n",
              " 'actual': 821,\n",
              " 'famous': 822,\n",
              " 'understand': 823,\n",
              " 'plan': 824,\n",
              " 'fails': 825,\n",
              " 'gun': 826,\n",
              " 'biggest': 827,\n",
              " 'follows': 828,\n",
              " 'billy': 829,\n",
              " 'jason': 830,\n",
              " 'steven': 831,\n",
              " 'cinema': 832,\n",
              " 'chase': 833,\n",
              " 'happened': 834,\n",
              " 'affair': 835,\n",
              " 'political': 836,\n",
              " '1997': 837,\n",
              " 'move': 838,\n",
              " 'perfectly': 839,\n",
              " 'felt': 840,\n",
              " 'party': 841,\n",
              " 'showing': 842,\n",
              " 'indeed': 843,\n",
              " 'cinematic': 844,\n",
              " 'focus': 845,\n",
              " 'follow': 846,\n",
              " 'light': 847,\n",
              " 'heard': 848,\n",
              " 'neither': 849,\n",
              " 'moves': 850,\n",
              " 'sounds': 851,\n",
              " 'female': 852,\n",
              " 'took': 853,\n",
              " 'ask': 854,\n",
              " 'jennifer': 855,\n",
              " 'wish': 856,\n",
              " 'filled': 857,\n",
              " 'anyway': 858,\n",
              " 'teenage': 859,\n",
              " 'french': 860,\n",
              " 'realize': 861,\n",
              " 'recommend': 862,\n",
              " 'dumb': 863,\n",
              " 'million': 864,\n",
              " 'depth': 865,\n",
              " 'boys': 866,\n",
              " 'julia': 867,\n",
              " 'humans': 868,\n",
              " 'air': 869,\n",
              " 'brian': 870,\n",
              " 'brilliant': 871,\n",
              " 'nick': 872,\n",
              " 'doubt': 873,\n",
              " 'wait': 874,\n",
              " 'states': 875,\n",
              " 'writers': 876,\n",
              " 'disaster': 877,\n",
              " 'asks': 878,\n",
              " 'company': 879,\n",
              " 'merely': 880,\n",
              " 'seriously': 881,\n",
              " '000': 882,\n",
              " 'particular': 883,\n",
              " 'believable': 884,\n",
              " 'sean': 885,\n",
              " 'thanks': 886,\n",
              " 'enjoyable': 887,\n",
              " 'slowly': 888,\n",
              " 'okay': 889,\n",
              " 'complex': 890,\n",
              " 'previous': 891,\n",
              " 'jay': 892,\n",
              " 'exciting': 893,\n",
              " 'conclusion': 894,\n",
              " 'sweet': 895,\n",
              " 'outside': 896,\n",
              " 'greatest': 897,\n",
              " 'points': 898,\n",
              " 'atmosphere': 899,\n",
              " 'street': 900,\n",
              " 'list': 901,\n",
              " 'constantly': 902,\n",
              " 'lots': 903,\n",
              " 'occasionally': 904,\n",
              " 'decent': 905,\n",
              " 'silly': 906,\n",
              " 'highly': 907,\n",
              " 'wars': 908,\n",
              " 'apartment': 909,\n",
              " 'island': 910,\n",
              " 'jamie': 911,\n",
              " 'expected': 912,\n",
              " 'delivers': 913,\n",
              " 'puts': 914,\n",
              " 'ability': 915,\n",
              " 'subplot': 916,\n",
              " 'older': 917,\n",
              " 'news': 918,\n",
              " 'view': 919,\n",
              " 'e': 920,\n",
              " 'development': 921,\n",
              " 'west': 922,\n",
              " 'havent': 923,\n",
              " 'pay': 924,\n",
              " 'typical': 925,\n",
              " 'doctor': 926,\n",
              " 'quick': 927,\n",
              " 'central': 928,\n",
              " 'sight': 929,\n",
              " 'purpose': 930,\n",
              " 'hate': 931,\n",
              " 'fear': 932,\n",
              " 'energy': 933,\n",
              " 'wrote': 934,\n",
              " 'memorable': 935,\n",
              " 'ten': 936,\n",
              " 'fire': 937,\n",
              " 'thinks': 938,\n",
              " 'fan': 939,\n",
              " 'state': 940,\n",
              " 'please': 941,\n",
              " 'intelligence': 942,\n",
              " 'flat': 943,\n",
              " 'class': 944,\n",
              " '2001': 945,\n",
              " 'force': 946,\n",
              " 'proves': 947,\n",
              " 'brought': 948,\n",
              " 'matt': 949,\n",
              " 'unlike': 950,\n",
              " 'ideas': 951,\n",
              " 'chemistry': 952,\n",
              " 'whatever': 953,\n",
              " 'climax': 954,\n",
              " 'budget': 955,\n",
              " 'soundtrack': 956,\n",
              " 'hits': 957,\n",
              " 'amount': 958,\n",
              " 'trouble': 959,\n",
              " 'impact': 960,\n",
              " 'college': 961,\n",
              " 'studio': 962,\n",
              " 'convincing': 963,\n",
              " 'cute': 964,\n",
              " 'killing': 965,\n",
              " 'willis': 966,\n",
              " 'animation': 967,\n",
              " 'cold': 968,\n",
              " 'thomas': 969,\n",
              " 'master': 970,\n",
              " 'hands': 971,\n",
              " 'favorite': 972,\n",
              " 'charles': 973,\n",
              " 'becoming': 974,\n",
              " 'leaving': 975,\n",
              " 'roger': 976,\n",
              " 'provide': 977,\n",
              " 'weve': 978,\n",
              " 'subtle': 979,\n",
              " 'setting': 980,\n",
              " 'scifi': 981,\n",
              " 'fairly': 982,\n",
              " 'brown': 983,\n",
              " 'dennis': 984,\n",
              " 'waste': 985,\n",
              " 'screenwriter': 986,\n",
              " 'thus': 987,\n",
              " 'student': 988,\n",
              " 'lies': 989,\n",
              " 'reasons': 990,\n",
              " 'detective': 991,\n",
              " 'adam': 992,\n",
              " 'fox': 993,\n",
              " 'army': 994,\n",
              " 'race': 995,\n",
              " 'mess': 996,\n",
              " 'stand': 997,\n",
              " 'ii': 998,\n",
              " 'interested': 999,\n",
              " 'gay': 1000,\n",
              " ...}"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNcbH9KkYcN5"
      },
      "source": [
        "Coverting our words to a sequence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_g_pEN7_ruD"
      },
      "source": [
        "train_sequences = tokenizer.texts_to_sequences(df.text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l_iCdqbX_zgP",
        "outputId": "42ad47b5-070f-46ea-eed1-2ca788891795"
      },
      "source": [
        "print(train_sequences[10:15])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1625, 11007, 1782, 1, 533, 8576, 12945, 583, 781, 222, 19, 29, 3950, 564, 8577, 1008, 1782, 832, 295, 82, 11008, 3765, 6465, 77, 1435, 3441, 70, 237, 2902, 3442, 21270, 5958, 21271, 2804, 1436, 704, 1497, 2213, 3017, 5182, 220, 12946, 1522, 5958, 5183, 2034, 207, 95, 781, 1250, 30, 62, 510, 2214, 2034, 794, 1219, 484, 9611, 15938, 5184, 4135, 39, 5547, 377, 5959, 209, 2214, 21272, 2908, 240, 972, 1437, 748, 2148, 3951, 2342, 243, 166, 4583, 1601, 141, 22, 1035, 9612, 125, 28, 249, 2343, 12947, 39, 279, 1924, 3766, 21273, 66, 16, 4584, 266, 2707, 84, 69, 3767, 1601, 531, 1469, 76, 77, 1435, 3441, 912, 1662, 479, 251, 1, 60, 1376, 517, 1319, 732, 371, 3145, 21274, 251, 135, 146, 21275, 18, 283, 173, 2909, 47, 3768, 21276, 77, 1435, 3441, 878, 517, 759, 913, 1045, 4585, 1224, 164, 4586, 10, 1251, 6466, 6467, 135, 12, 3146, 2, 108, 135, 766, 40, 54, 2342, 4135, 15938, 1038, 340, 958, 23, 3294, 6466, 518, 39, 279, 972, 1437, 16, 4584, 649, 222, 11, 70, 26, 65, 146, 4, 8578, 8, 5960, 41, 1298, 1523, 7056, 15939, 299, 5182, 119, 865, 663, 2610, 263, 2, 287, 1, 14, 70, 144, 269, 299, 1523, 149, 725, 15939, 75, 125, 7057, 258, 14, 301, 1435, 3441, 39, 1969, 1830, 4136, 307, 6468, 8579, 27, 913, 1663, 484, 85, 355, 18, 3, 7058, 4348, 1224, 1009, 759, 251, 385, 1524, 229, 16], [], [155, 1252, 327, 749, 622, 3952, 4856, 3295, 783, 17, 1316, 360, 5961, 21277, 8580, 220, 3296, 795, 53, 9613, 21278, 5185, 4587, 278, 1664, 7059, 3147, 1521, 11009, 5548, 1831, 21279, 2611, 100, 1275, 15940, 117, 53, 21280, 767, 1341, 187, 1047, 394, 21281, 6469, 9614, 16, 622, 3952, 1881, 7726, 633, 12948, 2708, 2612, 383, 2709, 15941, 52, 43, 33, 9615, 8581, 940, 3952, 141, 833, 32, 2708, 457, 10, 6, 176, 3297, 384, 6470, 634, 46, 2149, 21282, 1695, 70, 13, 223, 749, 7727, 3952, 71, 70, 45, 12949, 3953, 12950, 21283, 2270, 1665, 1438, 3298, 1, 197, 422, 104, 64, 2613, 21284, 147, 6, 1438, 65, 453, 7, 1, 5186, 39, 298, 1160, 12951, 768, 6471, 287, 62, 94, 125, 288, 27, 840, 374, 21285, 692, 274, 2, 6, 109, 1923, 141, 6, 288, 534, 458, 195, 21286, 4857, 36, 48, 783, 5549, 237, 3, 4588, 513, 2, 24, 18, 109, 550, 21287, 4857, 109, 550, 62, 146, 15942, 148, 9, 146, 19, 306, 576, 29, 462, 4857, 2710, 272, 233, 29, 12, 15943, 586, 9616, 166, 1666, 109, 250, 655, 5549, 48, 458, 4589, 5187, 1, 692, 792, 102, 351, 19, 3443, 264, 633, 5962, 102, 2614, 2708], [], [1553, 183, 485, 2344, 21288, 656, 155, 3954, 1371, 2520, 2345, 1342, 2910, 3020, 1094, 178, 129, 2805, 167, 1553, 1220, 253, 7728, 1253, 100, 3012, 255, 6472, 6473, 1498, 30, 134, 15944, 12952, 257, 141, 750, 26, 102, 351, 9617, 32, 54, 15945, 986, 32, 35, 29, 1925, 6474, 3444, 1556, 7060, 21289, 321, 1553, 418, 3148, 332, 11010, 2711, 1626, 1142, 2271, 12953, 2272, 7061, 5188, 21290, 472, 142, 37, 12954, 11, 15946, 77, 21291, 1, 4, 9618, 2521, 9619, 9620, 7062, 12955, 6461, 5963, 1371, 9621, 5189, 2910, 21292, 2150, 1094, 112, 15947, 6475, 15948, 4858, 1220, 253, 2806, 1832, 2273, 21293, 12956, 1525, 5964, 2345, 21294, 2522, 5550, 3769, 4349, 3596, 9622, 21295, 1220, 6473, 2615, 428, 38, 4348, 76, 12957, 12958, 601, 5963, 223, 21296, 1741, 21297, 891, 5190, 1371, 2085, 12959, 16, 3597, 497, 15949, 2523, 8582, 3149, 3445, 28, 5, 395, 41, 1048, 2712, 21298, 7063, 17, 348, 3598, 12937, 21299, 303, 36, 58, 1371, 101, 5963, 21300, 1343, 7064, 11011, 21301, 21302, 1198, 1276, 1827, 13, 163, 12960, 4590, 21303, 15950, 2910, 21304, 41, 8583, 1602, 115, 2512, 295, 225, 7729, 2346, 196, 178, 1199, 21305, 11012, 253, 46, 411, 973, 3150, 3955, 565, 15951, 430, 14, 96, 151, 5191, 3955, 565, 15951, 6476, 14, 12961, 1077, 664, 232, 253, 12962, 3762, 3599, 1403, 70, 2345, 239, 4137, 116, 2215, 7, 254, 17, 1627, 2150, 914, 556, 5964, 1883, 105, 5965, 7730, 4591, 3600, 1557, 8584, 281, 38, 3297, 261, 7065, 28, 8585, 21306, 21307, 401, 5963, 769, 806, 2345, 7062, 5963, 3299, 177, 155, 71, 2613, 5966, 3150, 1783, 273, 280, 11013, 21308, 12963, 2524, 9623, 315, 1553, 19, 2616, 15952, 15953, 442, 8586, 4138, 5, 2713, 4342, 146, 1143, 27, 132, 51, 3, 15954, 657, 339, 76, 28, 2712, 323, 7731, 807, 21309, 21310, 10985, 3770, 3, 959, 9624, 5963, 879, 326, 11, 176, 130, 2086, 21311, 139, 3771, 9625, 2617, 429, 3, 45, 5967, 3, 2440, 1220, 253, 12964, 864, 1624, 1016, 1553, 293, 9626, 820, 608, 1122, 873, 12965, 6477, 352, 2911, 21312, 4139, 3012, 255, 6472, 3772, 614, 6, 1603, 9627, 65, 887, 255, 6472, 3151, 43, 1181, 7732, 3601, 1200, 21313, 80, 12966, 21314, 713, 86, 180, 386, 1111, 11014, 15955, 12967, 4859, 954, 208, 3602, 11, 9628, 7733, 887, 6478, 112, 4140, 52, 325, 7060, 21315, 37, 504, 2347, 255, 6472, 21316, 504, 2347, 38, 4592, 215, 32, 699, 166, 44, 5968]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5WiWYd2pX8LZ"
      },
      "source": [
        "Padding our sequence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WV0G42s2AEs_"
      },
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# max number of wors in a sequence\n",
        "max_length = 20\n",
        "\n",
        "train_paded = pad_sequences(train_sequences, maxlen=max_length, padding=\"post\", truncating=\"post\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sJNWjSV6AfQi",
        "outputId": "dd3b815d-3b2a-4245-c11d-710d5726fb25"
      },
      "source": [
        "train_paded.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2266, 20)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nA5FovENYj3r"
      },
      "source": [
        "Cross checking padding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3KAuCOpjAjV9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "78394a34-1519-43f3-bc0e-59f66bb3a1c4"
      },
      "source": [
        "reverse_word_i = dict([(idx, word) for (word, idx) in word_index.items()])\n",
        "reverse_word_i[10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'story'"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IFsPpVMgA-7O"
      },
      "source": [
        "def decode(sequence):\n",
        "  return \" \".join([reverse_word_i.get(idx, \"?\") for idx in sequence])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QK6Df0jpBKVe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cba576e5-8ca1-4b01-ac7b-e21f5cadb169"
      },
      "source": [
        "decoded_text = decode(train_sequences[10])\n",
        "\n",
        "print(train_sequences[10])\n",
        "print(decoded_text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1625, 11007, 1782, 1, 533, 8576, 12945, 583, 781, 222, 19, 29, 3950, 564, 8577, 1008, 1782, 832, 295, 82, 11008, 3765, 6465, 77, 1435, 3441, 70, 237, 2902, 3442, 21270, 5958, 21271, 2804, 1436, 704, 1497, 2213, 3017, 5182, 220, 12946, 1522, 5958, 5183, 2034, 207, 95, 781, 1250, 30, 62, 510, 2214, 2034, 794, 1219, 484, 9611, 15938, 5184, 4135, 39, 5547, 377, 5959, 209, 2214, 21272, 2908, 240, 972, 1437, 748, 2148, 3951, 2342, 243, 166, 4583, 1601, 141, 22, 1035, 9612, 125, 28, 249, 2343, 12947, 39, 279, 1924, 3766, 21273, 66, 16, 4584, 266, 2707, 84, 69, 3767, 1601, 531, 1469, 76, 77, 1435, 3441, 912, 1662, 479, 251, 1, 60, 1376, 517, 1319, 732, 371, 3145, 21274, 251, 135, 146, 21275, 18, 283, 173, 2909, 47, 3768, 21276, 77, 1435, 3441, 878, 517, 759, 913, 1045, 4585, 1224, 164, 4586, 10, 1251, 6466, 6467, 135, 12, 3146, 2, 108, 135, 766, 40, 54, 2342, 4135, 15938, 1038, 340, 958, 23, 3294, 6466, 518, 39, 279, 972, 1437, 16, 4584, 649, 222, 11, 70, 26, 65, 146, 4, 8578, 8, 5960, 41, 1298, 1523, 7056, 15939, 299, 5182, 119, 865, 663, 2610, 263, 2, 287, 1, 14, 70, 144, 269, 299, 1523, 149, 725, 15939, 75, 125, 7057, 258, 14, 301, 1435, 3441, 39, 1969, 1830, 4136, 307, 6468, 8579, 27, 913, 1663, 484, 85, 355, 18, 3, 7058, 4348, 1224, 1009, 759, 251, 385, 1524, 229, 16]\n",
            "heres rarity childrens film attempts tackle weighty subject god done well could gem among wasteland modern childrens cinema unfortunately isnt jumbled messages unclear audience wide awake better left asleep fifth grader joshua beal joseph cross middle moral crisis beloved grandfather robert loggia died joshua begun quest wants find god discover bad things happen religious quest slightly disturbing parents dana delany denis leary best cope son explores different religious faiths catholic school favorite teacher sister terry rosie odonnell tries give guidance journey must make meanwhile momentous year life several adventures daredevil best friend dave timothy reifsnyder gets first crush begins wake world around spiritual journey somewhat confusing real audience wide awake expected surface appears kids film however deals serious issues likely boring todays instantgratification kids might seem heartening see someone trying produce something thoughtful kidvid audience wide awake asks serious questions delivers cheap gimmick answer bit meat story adults nostalgic bent might get kick movie actors might created great cast odonnell leary delany wasted roles amount little cameos nostalgic elements best friend favorite teacher first crush etc done much better movies actually seem like filler films strongest scenes touching flashbacks depicting joshuas relationship grandfather show depth present anywhere else movie maybe film would better instead playing relationship flashbacks set entirely joshuas last year grandpa certainly would entertaining wide awake best described failed experiment starts noble aspirations never delivers promise parents take children see one ought prepared answer tough questions kids arent bored death first\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KxtZHlk_Y5sJ"
      },
      "source": [
        "Train our model with one input layer, one hidden layer and one output layer.\n",
        "\n",
        "Our input data is simply vectors, and our labels are scalars (1 and 0). Two Dense layers with relu activations: Dense(20, activation='relu') and other with (10, activation=\"relu\")  as our hidden layer, And Dense(1, activation=\"sigmoid\") as our output layer.\n",
        "\n",
        "Word embedings give us an efficient, dense representation in whicb similar words have a similar encoding, and we wont have to specifiy that manually, so we will use the embeddings layer, which takes as input an integer matrix of size (batch, input_length).\n",
        "\n",
        "Also using 10-fold cross validation to optimize parameters using “accuracy” as the metric."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qiqKRDOtFqLE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2a0d6d0-247c-4785-cd9f-0c3eb060a9d1"
      },
      "source": [
        "from tensorflow.keras import layers, optimizers, losses, metrics\n",
        "import keras\n",
        "from sklearn.model_selection import KFold\n",
        "import numpy as np\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "labels = np.asarray(df['sentiment']).astype('float32')\n",
        "\n",
        "# Define the K-fold Cross Validator\n",
        "kfold = KFold(n_splits=10, shuffle=True)\n",
        "\n",
        "# Define per-fold score containers <-- these are new\n",
        "acc_per_fold = []\n",
        "loss_per_fold = []\n",
        "\n",
        "# K-fold Cross Validation model evaluation\n",
        "fold_no = 1\n",
        "for train, test in kfold.split(df[\"text\"], df[\"sentiment\"]):\n",
        "  model = keras.models.Sequential()\n",
        "\n",
        "  model.add(layers.Embedding(num_unique_words, 20, input_length=max_length)) #input layer\n",
        "  # model.add(layers.Dense(20, activation=\"relu\")) #input layer\n",
        "  # model.add(layers.Input(20)) #input layer\n",
        "  model.add(layers.Dense(20, activation=\"relu\")) #hidden layer\n",
        "  model.add(layers.Dense(10, activation=\"relu\")) #hidden layer 2\n",
        "  model.add(layers.Dense(1, activation=\"sigmoid\"))# output layer\n",
        "\n",
        "  model.compile(optimizer=optimizers.RMSprop(learning_rate=0.001),\n",
        "                loss=losses.binary_crossentropy,\n",
        "                metrics=[metrics.binary_accuracy])\n",
        "  \n",
        "    #create callback\n",
        "  filepath = 'my_best_model.hdf5'\n",
        "  checkpoint = ModelCheckpoint(filepath=filepath, \n",
        "                              monitor='loss',\n",
        "                              verbose=1, \n",
        "                              save_best_only=True,\n",
        "                              mode='min')\n",
        "  callbacks = [checkpoint]\n",
        "\n",
        "  # Fit data to model\n",
        "  history = model.fit(train_paded, labels,\n",
        "              batch_size=50,\n",
        "              epochs=20,\n",
        "              verbose=1,\n",
        "              callbacks=callbacks)\n",
        "  \n",
        "  # Generate generalization metrics\n",
        "  scores = model.evaluate(train_paded, labels, verbose=0)\n",
        "  print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
        "  acc_per_fold.append(scores[1] * 100)\n",
        "  loss_per_fold.append(scores[0])\n",
        "\n",
        "  # Increase fold number\n",
        "  fold_no = fold_no + 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "46/46 [==============================] - 1s 5ms/step - loss: 0.6933 - binary_accuracy: 0.5059\n",
            "\n",
            "Epoch 00001: loss improved from inf to 0.69330, saving model to my_best_model.hdf5\n",
            "Epoch 2/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.6917 - binary_accuracy: 0.5418\n",
            "\n",
            "Epoch 00002: loss improved from 0.69330 to 0.69171, saving model to my_best_model.hdf5\n",
            "Epoch 3/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.6889 - binary_accuracy: 0.5533\n",
            "\n",
            "Epoch 00003: loss improved from 0.69171 to 0.68887, saving model to my_best_model.hdf5\n",
            "Epoch 4/20\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.6829 - binary_accuracy: 0.5803\n",
            "\n",
            "Epoch 00004: loss improved from 0.68887 to 0.68289, saving model to my_best_model.hdf5\n",
            "Epoch 5/20\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.6720 - binary_accuracy: 0.5979\n",
            "\n",
            "Epoch 00005: loss improved from 0.68289 to 0.67199, saving model to my_best_model.hdf5\n",
            "Epoch 6/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.6604 - binary_accuracy: 0.5919\n",
            "\n",
            "Epoch 00006: loss improved from 0.67199 to 0.66041, saving model to my_best_model.hdf5\n",
            "Epoch 7/20\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.6477 - binary_accuracy: 0.6040\n",
            "\n",
            "Epoch 00007: loss improved from 0.66041 to 0.64773, saving model to my_best_model.hdf5\n",
            "Epoch 8/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.6359 - binary_accuracy: 0.6115\n",
            "\n",
            "Epoch 00008: loss improved from 0.64773 to 0.63588, saving model to my_best_model.hdf5\n",
            "Epoch 9/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.6264 - binary_accuracy: 0.6065\n",
            "\n",
            "Epoch 00009: loss improved from 0.63588 to 0.62640, saving model to my_best_model.hdf5\n",
            "Epoch 10/20\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.6199 - binary_accuracy: 0.5926\n",
            "\n",
            "Epoch 00010: loss improved from 0.62640 to 0.61987, saving model to my_best_model.hdf5\n",
            "Epoch 11/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.6143 - binary_accuracy: 0.6141\n",
            "\n",
            "Epoch 00011: loss improved from 0.61987 to 0.61435, saving model to my_best_model.hdf5\n",
            "Epoch 12/20\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.6109 - binary_accuracy: 0.5983\n",
            "\n",
            "Epoch 00012: loss improved from 0.61435 to 0.61089, saving model to my_best_model.hdf5\n",
            "Epoch 13/20\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.6074 - binary_accuracy: 0.6045\n",
            "\n",
            "Epoch 00013: loss improved from 0.61089 to 0.60737, saving model to my_best_model.hdf5\n",
            "Epoch 14/20\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.6049 - binary_accuracy: 0.5950\n",
            "\n",
            "Epoch 00014: loss improved from 0.60737 to 0.60491, saving model to my_best_model.hdf5\n",
            "Epoch 15/20\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.6021 - binary_accuracy: 0.6145\n",
            "\n",
            "Epoch 00015: loss improved from 0.60491 to 0.60214, saving model to my_best_model.hdf5\n",
            "Epoch 16/20\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.6011 - binary_accuracy: 0.5993\n",
            "\n",
            "Epoch 00016: loss improved from 0.60214 to 0.60115, saving model to my_best_model.hdf5\n",
            "Epoch 17/20\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.5987 - binary_accuracy: 0.6039\n",
            "\n",
            "Epoch 00017: loss improved from 0.60115 to 0.59870, saving model to my_best_model.hdf5\n",
            "Epoch 18/20\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.5970 - binary_accuracy: 0.5982\n",
            "\n",
            "Epoch 00018: loss improved from 0.59870 to 0.59704, saving model to my_best_model.hdf5\n",
            "Epoch 19/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.5946 - binary_accuracy: 0.6127\n",
            "\n",
            "Epoch 00019: loss improved from 0.59704 to 0.59465, saving model to my_best_model.hdf5\n",
            "Epoch 20/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.5940 - binary_accuracy: 0.6075\n",
            "\n",
            "Epoch 00020: loss improved from 0.59465 to 0.59400, saving model to my_best_model.hdf5\n",
            "Score for fold 1: loss of 0.5862818360328674; binary_accuracy of 61.63283586502075%\n",
            "Epoch 1/20\n",
            "46/46 [==============================] - 1s 5ms/step - loss: 0.6932 - binary_accuracy: 0.4818\n",
            "\n",
            "Epoch 00001: loss improved from inf to 0.69325, saving model to my_best_model.hdf5\n",
            "Epoch 2/20\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.6916 - binary_accuracy: 0.5412\n",
            "\n",
            "Epoch 00002: loss improved from 0.69325 to 0.69161, saving model to my_best_model.hdf5\n",
            "Epoch 3/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.6881 - binary_accuracy: 0.5466\n",
            "\n",
            "Epoch 00003: loss improved from 0.69161 to 0.68815, saving model to my_best_model.hdf5\n",
            "Epoch 4/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.6812 - binary_accuracy: 0.5811\n",
            "\n",
            "Epoch 00004: loss improved from 0.68815 to 0.68118, saving model to my_best_model.hdf5\n",
            "Epoch 5/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.6705 - binary_accuracy: 0.5866\n",
            "\n",
            "Epoch 00005: loss improved from 0.68118 to 0.67047, saving model to my_best_model.hdf5\n",
            "Epoch 6/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.6576 - binary_accuracy: 0.6013\n",
            "\n",
            "Epoch 00006: loss improved from 0.67047 to 0.65762, saving model to my_best_model.hdf5\n",
            "Epoch 7/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.6448 - binary_accuracy: 0.6034\n",
            "\n",
            "Epoch 00007: loss improved from 0.65762 to 0.64478, saving model to my_best_model.hdf5\n",
            "Epoch 8/20\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.6328 - binary_accuracy: 0.6131\n",
            "\n",
            "Epoch 00008: loss improved from 0.64478 to 0.63279, saving model to my_best_model.hdf5\n",
            "Epoch 9/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.6245 - binary_accuracy: 0.6080\n",
            "\n",
            "Epoch 00009: loss improved from 0.63279 to 0.62452, saving model to my_best_model.hdf5\n",
            "Epoch 10/20\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.6181 - binary_accuracy: 0.5962\n",
            "\n",
            "Epoch 00010: loss improved from 0.62452 to 0.61811, saving model to my_best_model.hdf5\n",
            "Epoch 11/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.6133 - binary_accuracy: 0.5851\n",
            "\n",
            "Epoch 00011: loss improved from 0.61811 to 0.61327, saving model to my_best_model.hdf5\n",
            "Epoch 12/20\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.6093 - binary_accuracy: 0.6004\n",
            "\n",
            "Epoch 00012: loss improved from 0.61327 to 0.60926, saving model to my_best_model.hdf5\n",
            "Epoch 13/20\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.6060 - binary_accuracy: 0.6119\n",
            "\n",
            "Epoch 00013: loss improved from 0.60926 to 0.60605, saving model to my_best_model.hdf5\n",
            "Epoch 14/20\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.6031 - binary_accuracy: 0.6073\n",
            "\n",
            "Epoch 00014: loss improved from 0.60605 to 0.60307, saving model to my_best_model.hdf5\n",
            "Epoch 15/20\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.6009 - binary_accuracy: 0.5957\n",
            "\n",
            "Epoch 00015: loss improved from 0.60307 to 0.60095, saving model to my_best_model.hdf5\n",
            "Epoch 16/20\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.5986 - binary_accuracy: 0.6146\n",
            "\n",
            "Epoch 00016: loss improved from 0.60095 to 0.59864, saving model to my_best_model.hdf5\n",
            "Epoch 17/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.5968 - binary_accuracy: 0.6115\n",
            "\n",
            "Epoch 00017: loss improved from 0.59864 to 0.59677, saving model to my_best_model.hdf5\n",
            "Epoch 18/20\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.5954 - binary_accuracy: 0.5983\n",
            "\n",
            "Epoch 00018: loss improved from 0.59677 to 0.59536, saving model to my_best_model.hdf5\n",
            "Epoch 19/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.5940 - binary_accuracy: 0.5960\n",
            "\n",
            "Epoch 00019: loss improved from 0.59536 to 0.59399, saving model to my_best_model.hdf5\n",
            "Epoch 20/20\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.5928 - binary_accuracy: 0.5942\n",
            "\n",
            "Epoch 00020: loss improved from 0.59399 to 0.59280, saving model to my_best_model.hdf5\n",
            "Score for fold 2: loss of 0.5855744481086731; binary_accuracy of 61.639440059661865%\n",
            "Epoch 1/20\n",
            "46/46 [==============================] - 1s 6ms/step - loss: 0.6933 - binary_accuracy: 0.4976\n",
            "\n",
            "Epoch 00001: loss improved from inf to 0.69326, saving model to my_best_model.hdf5\n",
            "Epoch 2/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.6918 - binary_accuracy: 0.5340\n",
            "\n",
            "Epoch 00002: loss improved from 0.69326 to 0.69182, saving model to my_best_model.hdf5\n",
            "Epoch 3/20\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.6895 - binary_accuracy: 0.5548\n",
            "\n",
            "Epoch 00003: loss improved from 0.69182 to 0.68946, saving model to my_best_model.hdf5\n",
            "Epoch 4/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.6841 - binary_accuracy: 0.5761\n",
            "\n",
            "Epoch 00004: loss improved from 0.68946 to 0.68406, saving model to my_best_model.hdf5\n",
            "Epoch 5/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.6750 - binary_accuracy: 0.5712\n",
            "\n",
            "Epoch 00005: loss improved from 0.68406 to 0.67498, saving model to my_best_model.hdf5\n",
            "Epoch 6/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.6621 - binary_accuracy: 0.6034\n",
            "\n",
            "Epoch 00006: loss improved from 0.67498 to 0.66215, saving model to my_best_model.hdf5\n",
            "Epoch 7/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.6480 - binary_accuracy: 0.6121\n",
            "\n",
            "Epoch 00007: loss improved from 0.66215 to 0.64802, saving model to my_best_model.hdf5\n",
            "Epoch 8/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.6359 - binary_accuracy: 0.5997\n",
            "\n",
            "Epoch 00008: loss improved from 0.64802 to 0.63586, saving model to my_best_model.hdf5\n",
            "Epoch 9/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.6257 - binary_accuracy: 0.6064\n",
            "\n",
            "Epoch 00009: loss improved from 0.63586 to 0.62569, saving model to my_best_model.hdf5\n",
            "Epoch 10/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.6184 - binary_accuracy: 0.6061\n",
            "\n",
            "Epoch 00010: loss improved from 0.62569 to 0.61844, saving model to my_best_model.hdf5\n",
            "Epoch 11/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.6135 - binary_accuracy: 0.6008\n",
            "\n",
            "Epoch 00011: loss improved from 0.61844 to 0.61347, saving model to my_best_model.hdf5\n",
            "Epoch 12/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.6091 - binary_accuracy: 0.6032\n",
            "\n",
            "Epoch 00012: loss improved from 0.61347 to 0.60914, saving model to my_best_model.hdf5\n",
            "Epoch 13/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.6057 - binary_accuracy: 0.6088\n",
            "\n",
            "Epoch 00013: loss improved from 0.60914 to 0.60567, saving model to my_best_model.hdf5\n",
            "Epoch 14/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.6031 - binary_accuracy: 0.6062\n",
            "\n",
            "Epoch 00014: loss improved from 0.60567 to 0.60306, saving model to my_best_model.hdf5\n",
            "Epoch 15/20\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.5999 - binary_accuracy: 0.6154\n",
            "\n",
            "Epoch 00015: loss improved from 0.60306 to 0.59985, saving model to my_best_model.hdf5\n",
            "Epoch 16/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.5988 - binary_accuracy: 0.5951\n",
            "\n",
            "Epoch 00016: loss improved from 0.59985 to 0.59876, saving model to my_best_model.hdf5\n",
            "Epoch 17/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.5964 - binary_accuracy: 0.5978\n",
            "\n",
            "Epoch 00017: loss improved from 0.59876 to 0.59636, saving model to my_best_model.hdf5\n",
            "Epoch 18/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.5949 - binary_accuracy: 0.5989\n",
            "\n",
            "Epoch 00018: loss improved from 0.59636 to 0.59486, saving model to my_best_model.hdf5\n",
            "Epoch 19/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.5926 - binary_accuracy: 0.6122\n",
            "\n",
            "Epoch 00019: loss improved from 0.59486 to 0.59257, saving model to my_best_model.hdf5\n",
            "Epoch 20/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.5919 - binary_accuracy: 0.6003\n",
            "\n",
            "Epoch 00020: loss improved from 0.59257 to 0.59191, saving model to my_best_model.hdf5\n",
            "Score for fold 3: loss of 0.5841214060783386; binary_accuracy of 61.66592836380005%\n",
            "Epoch 1/20\n",
            "46/46 [==============================] - 1s 5ms/step - loss: 0.6933 - binary_accuracy: 0.4990\n",
            "\n",
            "Epoch 00001: loss improved from inf to 0.69326, saving model to my_best_model.hdf5\n",
            "Epoch 2/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.6920 - binary_accuracy: 0.5380\n",
            "\n",
            "Epoch 00002: loss improved from 0.69326 to 0.69200, saving model to my_best_model.hdf5\n",
            "Epoch 3/20\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.6894 - binary_accuracy: 0.5517\n",
            "\n",
            "Epoch 00003: loss improved from 0.69200 to 0.68941, saving model to my_best_model.hdf5\n",
            "Epoch 4/20\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.6837 - binary_accuracy: 0.5695\n",
            "\n",
            "Epoch 00004: loss improved from 0.68941 to 0.68374, saving model to my_best_model.hdf5\n",
            "Epoch 5/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.6738 - binary_accuracy: 0.5831\n",
            "\n",
            "Epoch 00005: loss improved from 0.68374 to 0.67384, saving model to my_best_model.hdf5\n",
            "Epoch 6/20\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.6624 - binary_accuracy: 0.5976\n",
            "\n",
            "Epoch 00006: loss improved from 0.67384 to 0.66241, saving model to my_best_model.hdf5\n",
            "Epoch 7/20\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.6489 - binary_accuracy: 0.6040\n",
            "\n",
            "Epoch 00007: loss improved from 0.66241 to 0.64890, saving model to my_best_model.hdf5\n",
            "Epoch 8/20\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.6370 - binary_accuracy: 0.6010\n",
            "\n",
            "Epoch 00008: loss improved from 0.64890 to 0.63696, saving model to my_best_model.hdf5\n",
            "Epoch 9/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.6265 - binary_accuracy: 0.6195\n",
            "\n",
            "Epoch 00009: loss improved from 0.63696 to 0.62646, saving model to my_best_model.hdf5\n",
            "Epoch 10/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.6197 - binary_accuracy: 0.6110\n",
            "\n",
            "Epoch 00010: loss improved from 0.62646 to 0.61970, saving model to my_best_model.hdf5\n",
            "Epoch 11/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.6137 - binary_accuracy: 0.6017\n",
            "\n",
            "Epoch 00011: loss improved from 0.61970 to 0.61373, saving model to my_best_model.hdf5\n",
            "Epoch 12/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.6095 - binary_accuracy: 0.6111\n",
            "\n",
            "Epoch 00012: loss improved from 0.61373 to 0.60953, saving model to my_best_model.hdf5\n",
            "Epoch 13/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.6071 - binary_accuracy: 0.6021\n",
            "\n",
            "Epoch 00013: loss improved from 0.60953 to 0.60709, saving model to my_best_model.hdf5\n",
            "Epoch 14/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.6040 - binary_accuracy: 0.6008\n",
            "\n",
            "Epoch 00014: loss improved from 0.60709 to 0.60404, saving model to my_best_model.hdf5\n",
            "Epoch 15/20\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.6013 - binary_accuracy: 0.6079\n",
            "\n",
            "Epoch 00015: loss improved from 0.60404 to 0.60130, saving model to my_best_model.hdf5\n",
            "Epoch 16/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.5992 - binary_accuracy: 0.6049\n",
            "\n",
            "Epoch 00016: loss improved from 0.60130 to 0.59922, saving model to my_best_model.hdf5\n",
            "Epoch 17/20\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.5979 - binary_accuracy: 0.5966\n",
            "\n",
            "Epoch 00017: loss improved from 0.59922 to 0.59793, saving model to my_best_model.hdf5\n",
            "Epoch 18/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.5965 - binary_accuracy: 0.5959\n",
            "\n",
            "Epoch 00018: loss improved from 0.59793 to 0.59649, saving model to my_best_model.hdf5\n",
            "Epoch 19/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.5948 - binary_accuracy: 0.5979\n",
            "\n",
            "Epoch 00019: loss improved from 0.59649 to 0.59485, saving model to my_best_model.hdf5\n",
            "Epoch 20/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.5925 - binary_accuracy: 0.6102\n",
            "\n",
            "Epoch 00020: loss improved from 0.59485 to 0.59251, saving model to my_best_model.hdf5\n",
            "Score for fold 4: loss of 0.5884982943534851; binary_accuracy of 61.617374420166016%\n",
            "Epoch 1/20\n",
            "46/46 [==============================] - 1s 6ms/step - loss: 0.6933 - binary_accuracy: 0.4946\n",
            "\n",
            "Epoch 00001: loss improved from inf to 0.69334, saving model to my_best_model.hdf5\n",
            "Epoch 2/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.6918 - binary_accuracy: 0.5265\n",
            "\n",
            "Epoch 00002: loss improved from 0.69334 to 0.69180, saving model to my_best_model.hdf5\n",
            "Epoch 3/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.6890 - binary_accuracy: 0.5700\n",
            "\n",
            "Epoch 00003: loss improved from 0.69180 to 0.68903, saving model to my_best_model.hdf5\n",
            "Epoch 4/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.6837 - binary_accuracy: 0.5664\n",
            "\n",
            "Epoch 00004: loss improved from 0.68903 to 0.68372, saving model to my_best_model.hdf5\n",
            "Epoch 5/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.6759 - binary_accuracy: 0.5951\n",
            "\n",
            "Epoch 00005: loss improved from 0.68372 to 0.67585, saving model to my_best_model.hdf5\n",
            "Epoch 6/20\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.6647 - binary_accuracy: 0.5770\n",
            "\n",
            "Epoch 00006: loss improved from 0.67585 to 0.66473, saving model to my_best_model.hdf5\n",
            "Epoch 7/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.6515 - binary_accuracy: 0.6058\n",
            "\n",
            "Epoch 00007: loss improved from 0.66473 to 0.65155, saving model to my_best_model.hdf5\n",
            "Epoch 8/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.6391 - binary_accuracy: 0.6008\n",
            "\n",
            "Epoch 00008: loss improved from 0.65155 to 0.63910, saving model to my_best_model.hdf5\n",
            "Epoch 9/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.6288 - binary_accuracy: 0.6058\n",
            "\n",
            "Epoch 00009: loss improved from 0.63910 to 0.62881, saving model to my_best_model.hdf5\n",
            "Epoch 10/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.6207 - binary_accuracy: 0.6092\n",
            "\n",
            "Epoch 00010: loss improved from 0.62881 to 0.62065, saving model to my_best_model.hdf5\n",
            "Epoch 11/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.6150 - binary_accuracy: 0.6015\n",
            "\n",
            "Epoch 00011: loss improved from 0.62065 to 0.61496, saving model to my_best_model.hdf5\n",
            "Epoch 12/20\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.6102 - binary_accuracy: 0.5977\n",
            "\n",
            "Epoch 00012: loss improved from 0.61496 to 0.61017, saving model to my_best_model.hdf5\n",
            "Epoch 13/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.6069 - binary_accuracy: 0.5987\n",
            "\n",
            "Epoch 00013: loss improved from 0.61017 to 0.60690, saving model to my_best_model.hdf5\n",
            "Epoch 14/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.6042 - binary_accuracy: 0.6043\n",
            "\n",
            "Epoch 00014: loss improved from 0.60690 to 0.60421, saving model to my_best_model.hdf5\n",
            "Epoch 15/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.6016 - binary_accuracy: 0.5981\n",
            "\n",
            "Epoch 00015: loss improved from 0.60421 to 0.60159, saving model to my_best_model.hdf5\n",
            "Epoch 16/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.5995 - binary_accuracy: 0.6035\n",
            "\n",
            "Epoch 00016: loss improved from 0.60159 to 0.59950, saving model to my_best_model.hdf5\n",
            "Epoch 17/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.5976 - binary_accuracy: 0.5981\n",
            "\n",
            "Epoch 00017: loss improved from 0.59950 to 0.59757, saving model to my_best_model.hdf5\n",
            "Epoch 18/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.5960 - binary_accuracy: 0.6040\n",
            "\n",
            "Epoch 00018: loss improved from 0.59757 to 0.59604, saving model to my_best_model.hdf5\n",
            "Epoch 19/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.5944 - binary_accuracy: 0.6052\n",
            "\n",
            "Epoch 00019: loss improved from 0.59604 to 0.59438, saving model to my_best_model.hdf5\n",
            "Epoch 20/20\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.5928 - binary_accuracy: 0.6019\n",
            "\n",
            "Epoch 00020: loss improved from 0.59438 to 0.59284, saving model to my_best_model.hdf5\n",
            "Score for fold 5: loss of 0.585228443145752; binary_accuracy of 61.648279428482056%\n",
            "Epoch 1/20\n",
            "46/46 [==============================] - 1s 5ms/step - loss: 0.6936 - binary_accuracy: 0.4961\n",
            "\n",
            "Epoch 00001: loss improved from inf to 0.69356, saving model to my_best_model.hdf5\n",
            "Epoch 2/20\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.6911 - binary_accuracy: 0.5424\n",
            "\n",
            "Epoch 00002: loss improved from 0.69356 to 0.69109, saving model to my_best_model.hdf5\n",
            "Epoch 3/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.6865 - binary_accuracy: 0.5720\n",
            "\n",
            "Epoch 00003: loss improved from 0.69109 to 0.68652, saving model to my_best_model.hdf5\n",
            "Epoch 4/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.6784 - binary_accuracy: 0.5767\n",
            "\n",
            "Epoch 00004: loss improved from 0.68652 to 0.67840, saving model to my_best_model.hdf5\n",
            "Epoch 5/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.6660 - binary_accuracy: 0.5965\n",
            "\n",
            "Epoch 00005: loss improved from 0.67840 to 0.66604, saving model to my_best_model.hdf5\n",
            "Epoch 6/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.6526 - binary_accuracy: 0.6054\n",
            "\n",
            "Epoch 00006: loss improved from 0.66604 to 0.65263, saving model to my_best_model.hdf5\n",
            "Epoch 7/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.6401 - binary_accuracy: 0.6081\n",
            "\n",
            "Epoch 00007: loss improved from 0.65263 to 0.64007, saving model to my_best_model.hdf5\n",
            "Epoch 8/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.6295 - binary_accuracy: 0.6020\n",
            "\n",
            "Epoch 00008: loss improved from 0.64007 to 0.62949, saving model to my_best_model.hdf5\n",
            "Epoch 9/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.6214 - binary_accuracy: 0.6028\n",
            "\n",
            "Epoch 00009: loss improved from 0.62949 to 0.62136, saving model to my_best_model.hdf5\n",
            "Epoch 10/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.6159 - binary_accuracy: 0.6064\n",
            "\n",
            "Epoch 00010: loss improved from 0.62136 to 0.61585, saving model to my_best_model.hdf5\n",
            "Epoch 11/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.6107 - binary_accuracy: 0.6126\n",
            "\n",
            "Epoch 00011: loss improved from 0.61585 to 0.61070, saving model to my_best_model.hdf5\n",
            "Epoch 12/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.6080 - binary_accuracy: 0.6047\n",
            "\n",
            "Epoch 00012: loss improved from 0.61070 to 0.60798, saving model to my_best_model.hdf5\n",
            "Epoch 13/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.6058 - binary_accuracy: 0.6051\n",
            "\n",
            "Epoch 00013: loss improved from 0.60798 to 0.60577, saving model to my_best_model.hdf5\n",
            "Epoch 14/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.6031 - binary_accuracy: 0.5984\n",
            "\n",
            "Epoch 00014: loss improved from 0.60577 to 0.60312, saving model to my_best_model.hdf5\n",
            "Epoch 15/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.6000 - binary_accuracy: 0.6034\n",
            "\n",
            "Epoch 00015: loss improved from 0.60312 to 0.60000, saving model to my_best_model.hdf5\n",
            "Epoch 16/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.5997 - binary_accuracy: 0.5943\n",
            "\n",
            "Epoch 00016: loss improved from 0.60000 to 0.59970, saving model to my_best_model.hdf5\n",
            "Epoch 17/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.5969 - binary_accuracy: 0.6083\n",
            "\n",
            "Epoch 00017: loss improved from 0.59970 to 0.59689, saving model to my_best_model.hdf5\n",
            "Epoch 18/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.5956 - binary_accuracy: 0.6132\n",
            "\n",
            "Epoch 00018: loss improved from 0.59689 to 0.59561, saving model to my_best_model.hdf5\n",
            "Epoch 19/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.5944 - binary_accuracy: 0.5989\n",
            "\n",
            "Epoch 00019: loss improved from 0.59561 to 0.59443, saving model to my_best_model.hdf5\n",
            "Epoch 20/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.5926 - binary_accuracy: 0.6152\n",
            "\n",
            "Epoch 00020: loss improved from 0.59443 to 0.59260, saving model to my_best_model.hdf5\n",
            "Score for fold 6: loss of 0.586604654788971; binary_accuracy of 61.61738634109497%\n",
            "Epoch 1/20\n",
            "46/46 [==============================] - 1s 6ms/step - loss: 0.6934 - binary_accuracy: 0.4947\n",
            "\n",
            "Epoch 00001: loss improved from inf to 0.69337, saving model to my_best_model.hdf5\n",
            "Epoch 2/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.6917 - binary_accuracy: 0.5400\n",
            "\n",
            "Epoch 00002: loss improved from 0.69337 to 0.69165, saving model to my_best_model.hdf5\n",
            "Epoch 3/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.6886 - binary_accuracy: 0.5605\n",
            "\n",
            "Epoch 00003: loss improved from 0.69165 to 0.68861, saving model to my_best_model.hdf5\n",
            "Epoch 4/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.6826 - binary_accuracy: 0.5757\n",
            "\n",
            "Epoch 00004: loss improved from 0.68861 to 0.68260, saving model to my_best_model.hdf5\n",
            "Epoch 5/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.6715 - binary_accuracy: 0.5912\n",
            "\n",
            "Epoch 00005: loss improved from 0.68260 to 0.67153, saving model to my_best_model.hdf5\n",
            "Epoch 6/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.6576 - binary_accuracy: 0.6033\n",
            "\n",
            "Epoch 00006: loss improved from 0.67153 to 0.65757, saving model to my_best_model.hdf5\n",
            "Epoch 7/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.6445 - binary_accuracy: 0.5973\n",
            "\n",
            "Epoch 00007: loss improved from 0.65757 to 0.64450, saving model to my_best_model.hdf5\n",
            "Epoch 8/20\n",
            "46/46 [==============================] - 0s 7ms/step - loss: 0.6329 - binary_accuracy: 0.6099\n",
            "\n",
            "Epoch 00008: loss improved from 0.64450 to 0.63285, saving model to my_best_model.hdf5\n",
            "Epoch 9/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.6237 - binary_accuracy: 0.6151\n",
            "\n",
            "Epoch 00009: loss improved from 0.63285 to 0.62366, saving model to my_best_model.hdf5\n",
            "Epoch 10/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.6178 - binary_accuracy: 0.6095\n",
            "\n",
            "Epoch 00010: loss improved from 0.62366 to 0.61781, saving model to my_best_model.hdf5\n",
            "Epoch 11/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.6122 - binary_accuracy: 0.6120\n",
            "\n",
            "Epoch 00011: loss improved from 0.61781 to 0.61218, saving model to my_best_model.hdf5\n",
            "Epoch 12/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.6084 - binary_accuracy: 0.6090\n",
            "\n",
            "Epoch 00012: loss improved from 0.61218 to 0.60836, saving model to my_best_model.hdf5\n",
            "Epoch 13/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.6054 - binary_accuracy: 0.6036\n",
            "\n",
            "Epoch 00013: loss improved from 0.60836 to 0.60541, saving model to my_best_model.hdf5\n",
            "Epoch 14/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.6031 - binary_accuracy: 0.6007\n",
            "\n",
            "Epoch 00014: loss improved from 0.60541 to 0.60306, saving model to my_best_model.hdf5\n",
            "Epoch 15/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.6009 - binary_accuracy: 0.6077\n",
            "\n",
            "Epoch 00015: loss improved from 0.60306 to 0.60089, saving model to my_best_model.hdf5\n",
            "Epoch 16/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.5988 - binary_accuracy: 0.6032\n",
            "\n",
            "Epoch 00016: loss improved from 0.60089 to 0.59879, saving model to my_best_model.hdf5\n",
            "Epoch 17/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.5973 - binary_accuracy: 0.5992\n",
            "\n",
            "Epoch 00017: loss improved from 0.59879 to 0.59735, saving model to my_best_model.hdf5\n",
            "Epoch 18/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.5953 - binary_accuracy: 0.5998\n",
            "\n",
            "Epoch 00018: loss improved from 0.59735 to 0.59530, saving model to my_best_model.hdf5\n",
            "Epoch 19/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.5940 - binary_accuracy: 0.6037\n",
            "\n",
            "Epoch 00019: loss improved from 0.59530 to 0.59404, saving model to my_best_model.hdf5\n",
            "Epoch 20/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.5932 - binary_accuracy: 0.5917\n",
            "\n",
            "Epoch 00020: loss improved from 0.59404 to 0.59323, saving model to my_best_model.hdf5\n",
            "Score for fold 7: loss of 0.5849563479423523; binary_accuracy of 61.67254447937012%\n",
            "Epoch 1/20\n",
            "46/46 [==============================] - 1s 6ms/step - loss: 0.6932 - binary_accuracy: 0.4970\n",
            "\n",
            "Epoch 00001: loss improved from inf to 0.69321, saving model to my_best_model.hdf5\n",
            "Epoch 2/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.6919 - binary_accuracy: 0.5378\n",
            "\n",
            "Epoch 00002: loss improved from 0.69321 to 0.69186, saving model to my_best_model.hdf5\n",
            "Epoch 3/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.6892 - binary_accuracy: 0.5612\n",
            "\n",
            "Epoch 00003: loss improved from 0.69186 to 0.68922, saving model to my_best_model.hdf5\n",
            "Epoch 4/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.6839 - binary_accuracy: 0.5715\n",
            "\n",
            "Epoch 00004: loss improved from 0.68922 to 0.68391, saving model to my_best_model.hdf5\n",
            "Epoch 5/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.6750 - binary_accuracy: 0.5924\n",
            "\n",
            "Epoch 00005: loss improved from 0.68391 to 0.67503, saving model to my_best_model.hdf5\n",
            "Epoch 6/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.6624 - binary_accuracy: 0.5937\n",
            "\n",
            "Epoch 00006: loss improved from 0.67503 to 0.66243, saving model to my_best_model.hdf5\n",
            "Epoch 7/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.6488 - binary_accuracy: 0.5942\n",
            "\n",
            "Epoch 00007: loss improved from 0.66243 to 0.64877, saving model to my_best_model.hdf5\n",
            "Epoch 8/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.6368 - binary_accuracy: 0.6071\n",
            "\n",
            "Epoch 00008: loss improved from 0.64877 to 0.63683, saving model to my_best_model.hdf5\n",
            "Epoch 9/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.6267 - binary_accuracy: 0.6123\n",
            "\n",
            "Epoch 00009: loss improved from 0.63683 to 0.62672, saving model to my_best_model.hdf5\n",
            "Epoch 10/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.6184 - binary_accuracy: 0.6153\n",
            "\n",
            "Epoch 00010: loss improved from 0.62672 to 0.61836, saving model to my_best_model.hdf5\n",
            "Epoch 11/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.6136 - binary_accuracy: 0.6029\n",
            "\n",
            "Epoch 00011: loss improved from 0.61836 to 0.61365, saving model to my_best_model.hdf5\n",
            "Epoch 12/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.6097 - binary_accuracy: 0.6077\n",
            "\n",
            "Epoch 00012: loss improved from 0.61365 to 0.60972, saving model to my_best_model.hdf5\n",
            "Epoch 13/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.6056 - binary_accuracy: 0.6139\n",
            "\n",
            "Epoch 00013: loss improved from 0.60972 to 0.60556, saving model to my_best_model.hdf5\n",
            "Epoch 14/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.6023 - binary_accuracy: 0.6201\n",
            "\n",
            "Epoch 00014: loss improved from 0.60556 to 0.60234, saving model to my_best_model.hdf5\n",
            "Epoch 15/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.6004 - binary_accuracy: 0.6056\n",
            "\n",
            "Epoch 00015: loss improved from 0.60234 to 0.60044, saving model to my_best_model.hdf5\n",
            "Epoch 16/20\n",
            "46/46 [==============================] - 0s 7ms/step - loss: 0.5982 - binary_accuracy: 0.6086\n",
            "\n",
            "Epoch 00016: loss improved from 0.60044 to 0.59821, saving model to my_best_model.hdf5\n",
            "Epoch 17/20\n",
            "46/46 [==============================] - 0s 7ms/step - loss: 0.5964 - binary_accuracy: 0.6059\n",
            "\n",
            "Epoch 00017: loss improved from 0.59821 to 0.59644, saving model to my_best_model.hdf5\n",
            "Epoch 18/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.5945 - binary_accuracy: 0.6094\n",
            "\n",
            "Epoch 00018: loss improved from 0.59644 to 0.59446, saving model to my_best_model.hdf5\n",
            "Epoch 19/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.5931 - binary_accuracy: 0.5916\n",
            "\n",
            "Epoch 00019: loss improved from 0.59446 to 0.59309, saving model to my_best_model.hdf5\n",
            "Epoch 20/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.5917 - binary_accuracy: 0.6006\n",
            "\n",
            "Epoch 00020: loss improved from 0.59309 to 0.59174, saving model to my_best_model.hdf5\n",
            "Score for fold 8: loss of 0.5844037532806396; binary_accuracy of 61.648279428482056%\n",
            "Epoch 1/20\n",
            "46/46 [==============================] - 1s 5ms/step - loss: 0.6934 - binary_accuracy: 0.4888\n",
            "\n",
            "Epoch 00001: loss improved from inf to 0.69336, saving model to my_best_model.hdf5\n",
            "Epoch 2/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.6913 - binary_accuracy: 0.5373\n",
            "\n",
            "Epoch 00002: loss improved from 0.69336 to 0.69132, saving model to my_best_model.hdf5\n",
            "Epoch 3/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.6874 - binary_accuracy: 0.5510\n",
            "\n",
            "Epoch 00003: loss improved from 0.69132 to 0.68744, saving model to my_best_model.hdf5\n",
            "Epoch 4/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.6796 - binary_accuracy: 0.5912\n",
            "\n",
            "Epoch 00004: loss improved from 0.68744 to 0.67960, saving model to my_best_model.hdf5\n",
            "Epoch 5/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.6693 - binary_accuracy: 0.5935\n",
            "\n",
            "Epoch 00005: loss improved from 0.67960 to 0.66929, saving model to my_best_model.hdf5\n",
            "Epoch 6/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.6558 - binary_accuracy: 0.5909\n",
            "\n",
            "Epoch 00006: loss improved from 0.66929 to 0.65576, saving model to my_best_model.hdf5\n",
            "Epoch 7/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.6414 - binary_accuracy: 0.6203\n",
            "\n",
            "Epoch 00007: loss improved from 0.65576 to 0.64137, saving model to my_best_model.hdf5\n",
            "Epoch 8/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.6304 - binary_accuracy: 0.6088\n",
            "\n",
            "Epoch 00008: loss improved from 0.64137 to 0.63042, saving model to my_best_model.hdf5\n",
            "Epoch 9/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.6220 - binary_accuracy: 0.6060\n",
            "\n",
            "Epoch 00009: loss improved from 0.63042 to 0.62202, saving model to my_best_model.hdf5\n",
            "Epoch 10/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.6155 - binary_accuracy: 0.6153\n",
            "\n",
            "Epoch 00010: loss improved from 0.62202 to 0.61550, saving model to my_best_model.hdf5\n",
            "Epoch 11/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.6093 - binary_accuracy: 0.6258\n",
            "\n",
            "Epoch 00011: loss improved from 0.61550 to 0.60931, saving model to my_best_model.hdf5\n",
            "Epoch 12/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.6093 - binary_accuracy: 0.6032\n",
            "\n",
            "Epoch 00012: loss improved from 0.60931 to 0.60930, saving model to my_best_model.hdf5\n",
            "Epoch 13/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.6045 - binary_accuracy: 0.6146\n",
            "\n",
            "Epoch 00013: loss improved from 0.60930 to 0.60451, saving model to my_best_model.hdf5\n",
            "Epoch 14/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.6024 - binary_accuracy: 0.6117\n",
            "\n",
            "Epoch 00014: loss improved from 0.60451 to 0.60240, saving model to my_best_model.hdf5\n",
            "Epoch 15/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.5995 - binary_accuracy: 0.6171\n",
            "\n",
            "Epoch 00015: loss improved from 0.60240 to 0.59954, saving model to my_best_model.hdf5\n",
            "Epoch 16/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.5986 - binary_accuracy: 0.6073\n",
            "\n",
            "Epoch 00016: loss improved from 0.59954 to 0.59856, saving model to my_best_model.hdf5\n",
            "Epoch 17/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.5966 - binary_accuracy: 0.6072\n",
            "\n",
            "Epoch 00017: loss improved from 0.59856 to 0.59658, saving model to my_best_model.hdf5\n",
            "Epoch 18/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.5956 - binary_accuracy: 0.5948\n",
            "\n",
            "Epoch 00018: loss improved from 0.59658 to 0.59557, saving model to my_best_model.hdf5\n",
            "Epoch 19/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.5936 - binary_accuracy: 0.6038\n",
            "\n",
            "Epoch 00019: loss improved from 0.59557 to 0.59363, saving model to my_best_model.hdf5\n",
            "Epoch 20/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.5915 - binary_accuracy: 0.6182\n",
            "\n",
            "Epoch 00020: loss improved from 0.59363 to 0.59147, saving model to my_best_model.hdf5\n",
            "Score for fold 9: loss of 0.5845116376876831; binary_accuracy of 61.699020862579346%\n",
            "Epoch 1/20\n",
            "46/46 [==============================] - 1s 6ms/step - loss: 0.6934 - binary_accuracy: 0.4882\n",
            "\n",
            "Epoch 00001: loss improved from inf to 0.69342, saving model to my_best_model.hdf5\n",
            "Epoch 2/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.6916 - binary_accuracy: 0.5292\n",
            "\n",
            "Epoch 00002: loss improved from 0.69342 to 0.69162, saving model to my_best_model.hdf5\n",
            "Epoch 3/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.6884 - binary_accuracy: 0.5498\n",
            "\n",
            "Epoch 00003: loss improved from 0.69162 to 0.68841, saving model to my_best_model.hdf5\n",
            "Epoch 4/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.6822 - binary_accuracy: 0.5752\n",
            "\n",
            "Epoch 00004: loss improved from 0.68841 to 0.68222, saving model to my_best_model.hdf5\n",
            "Epoch 5/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.6717 - binary_accuracy: 0.5965\n",
            "\n",
            "Epoch 00005: loss improved from 0.68222 to 0.67172, saving model to my_best_model.hdf5\n",
            "Epoch 6/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.6590 - binary_accuracy: 0.6048\n",
            "\n",
            "Epoch 00006: loss improved from 0.67172 to 0.65899, saving model to my_best_model.hdf5\n",
            "Epoch 7/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.6461 - binary_accuracy: 0.6058\n",
            "\n",
            "Epoch 00007: loss improved from 0.65899 to 0.64611, saving model to my_best_model.hdf5\n",
            "Epoch 8/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.6346 - binary_accuracy: 0.6114\n",
            "\n",
            "Epoch 00008: loss improved from 0.64611 to 0.63460, saving model to my_best_model.hdf5\n",
            "Epoch 9/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.6275 - binary_accuracy: 0.6133\n",
            "\n",
            "Epoch 00009: loss improved from 0.63460 to 0.62746, saving model to my_best_model.hdf5\n",
            "Epoch 10/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.6201 - binary_accuracy: 0.6092\n",
            "\n",
            "Epoch 00010: loss improved from 0.62746 to 0.62008, saving model to my_best_model.hdf5\n",
            "Epoch 11/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.6153 - binary_accuracy: 0.5989\n",
            "\n",
            "Epoch 00011: loss improved from 0.62008 to 0.61531, saving model to my_best_model.hdf5\n",
            "Epoch 12/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.6109 - binary_accuracy: 0.6069\n",
            "\n",
            "Epoch 00012: loss improved from 0.61531 to 0.61090, saving model to my_best_model.hdf5\n",
            "Epoch 13/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.6078 - binary_accuracy: 0.6138\n",
            "\n",
            "Epoch 00013: loss improved from 0.61090 to 0.60777, saving model to my_best_model.hdf5\n",
            "Epoch 14/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.6046 - binary_accuracy: 0.6085\n",
            "\n",
            "Epoch 00014: loss improved from 0.60777 to 0.60455, saving model to my_best_model.hdf5\n",
            "Epoch 15/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.6022 - binary_accuracy: 0.6034\n",
            "\n",
            "Epoch 00015: loss improved from 0.60455 to 0.60217, saving model to my_best_model.hdf5\n",
            "Epoch 16/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.6013 - binary_accuracy: 0.5990\n",
            "\n",
            "Epoch 00016: loss improved from 0.60217 to 0.60129, saving model to my_best_model.hdf5\n",
            "Epoch 17/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.5978 - binary_accuracy: 0.6173\n",
            "\n",
            "Epoch 00017: loss improved from 0.60129 to 0.59777, saving model to my_best_model.hdf5\n",
            "Epoch 18/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.5973 - binary_accuracy: 0.6015\n",
            "\n",
            "Epoch 00018: loss improved from 0.59777 to 0.59728, saving model to my_best_model.hdf5\n",
            "Epoch 19/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.5956 - binary_accuracy: 0.6088\n",
            "\n",
            "Epoch 00019: loss improved from 0.59728 to 0.59560, saving model to my_best_model.hdf5\n",
            "Epoch 20/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.5928 - binary_accuracy: 0.6169\n",
            "\n",
            "Epoch 00020: loss improved from 0.59560 to 0.59285, saving model to my_best_model.hdf5\n",
            "Score for fold 10: loss of 0.5862624645233154; binary_accuracy of 61.65268421173096%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBe_KKaEbXss"
      },
      "source": [
        "Parameter optimization flow: I first tried using a normal input layer a dense layer as my input layer, but realized my accuracy and loss was better with an embedding layer as my input layer, then i had to go back and calculate the needed hyperparameters for an ebedding layer.\n",
        "\n",
        "I also had issues with the number of nodes to assign my layers, but finally settled on the of the input X"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gPB7_oz_bihu"
      },
      "source": [
        "Parameters for the best model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TBf4dRhMQ5bh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "109ac487-17d6-4608-f844-88f4bff0df6a"
      },
      "source": [
        "from keras.models import Sequential, load_model\n",
        "\n",
        "model = load_model(\"my_best_model.hdf5\")\n",
        "model.get_config()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'layers': [{'class_name': 'InputLayer',\n",
              "   'config': {'batch_input_shape': (None, 20),\n",
              "    'dtype': 'float32',\n",
              "    'name': 'embedding_29_input',\n",
              "    'ragged': False,\n",
              "    'sparse': False}},\n",
              "  {'class_name': 'Embedding',\n",
              "   'config': {'activity_regularizer': None,\n",
              "    'batch_input_shape': (None, 20),\n",
              "    'dtype': 'float32',\n",
              "    'embeddings_constraint': None,\n",
              "    'embeddings_initializer': {'class_name': 'RandomUniform',\n",
              "     'config': {'maxval': 0.05, 'minval': -0.05, 'seed': None}},\n",
              "    'embeddings_regularizer': None,\n",
              "    'input_dim': 37049,\n",
              "    'input_length': 20,\n",
              "    'mask_zero': False,\n",
              "    'name': 'embedding_29',\n",
              "    'output_dim': 20,\n",
              "    'trainable': True}},\n",
              "  {'class_name': 'Dense',\n",
              "   'config': {'activation': 'relu',\n",
              "    'activity_regularizer': None,\n",
              "    'bias_constraint': None,\n",
              "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
              "    'bias_regularizer': None,\n",
              "    'dtype': 'float32',\n",
              "    'kernel_constraint': None,\n",
              "    'kernel_initializer': {'class_name': 'GlorotUniform',\n",
              "     'config': {'seed': None}},\n",
              "    'kernel_regularizer': None,\n",
              "    'name': 'dense_157',\n",
              "    'trainable': True,\n",
              "    'units': 20,\n",
              "    'use_bias': True}},\n",
              "  {'class_name': 'Dense',\n",
              "   'config': {'activation': 'relu',\n",
              "    'activity_regularizer': None,\n",
              "    'bias_constraint': None,\n",
              "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
              "    'bias_regularizer': None,\n",
              "    'dtype': 'float32',\n",
              "    'kernel_constraint': None,\n",
              "    'kernel_initializer': {'class_name': 'GlorotUniform',\n",
              "     'config': {'seed': None}},\n",
              "    'kernel_regularizer': None,\n",
              "    'name': 'dense_158',\n",
              "    'trainable': True,\n",
              "    'units': 10,\n",
              "    'use_bias': True}},\n",
              "  {'class_name': 'Dense',\n",
              "   'config': {'activation': 'sigmoid',\n",
              "    'activity_regularizer': None,\n",
              "    'bias_constraint': None,\n",
              "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
              "    'bias_regularizer': None,\n",
              "    'dtype': 'float32',\n",
              "    'kernel_constraint': None,\n",
              "    'kernel_initializer': {'class_name': 'GlorotUniform',\n",
              "     'config': {'seed': None}},\n",
              "    'kernel_regularizer': None,\n",
              "    'name': 'dense_159',\n",
              "    'trainable': True,\n",
              "    'units': 1,\n",
              "    'use_bias': True}}],\n",
              " 'name': 'sequential_49'}"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N9FreOYJbLtG"
      },
      "source": [
        " **2.2 Test your feed-forward Neural Network**\n",
        "Your goal for this part of the assignment is to test your neural network on the “training set”. \n",
        "5 \n",
        " \n",
        "1. Use the parameters from best performing model in Section 2.1 of this assignment and \n",
        "train the neural network on your whole training corpus. \n",
        "2. Report your accuracy on the entire training set. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WcyjfM5WSHjz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10624488-2055-4c05-b7e8-93e839aa997f"
      },
      "source": [
        "best_model = load_model(\"my_best_model.hdf5\")\n",
        "\n",
        "# Fit data to model\n",
        "history = best_model.fit(train_paded, labels,\n",
        "            batch_size=50,\n",
        "            epochs=20,\n",
        "            verbose=1)\n",
        "\n",
        "# Generate generalization metrics\n",
        "scores = best_model.evaluate(train_paded, labels, verbose=0)\n",
        "print(scores)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "46/46 [==============================] - 1s 6ms/step - loss: 0.5911 - binary_accuracy: 0.6054\n",
            "Epoch 2/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.5900 - binary_accuracy: 0.6043\n",
            "Epoch 3/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.5885 - binary_accuracy: 0.6203\n",
            "Epoch 4/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.5878 - binary_accuracy: 0.6061\n",
            "Epoch 5/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.5870 - binary_accuracy: 0.6057\n",
            "Epoch 6/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.5858 - binary_accuracy: 0.6131\n",
            "Epoch 7/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.5851 - binary_accuracy: 0.6120\n",
            "Epoch 8/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.5846 - binary_accuracy: 0.6122\n",
            "Epoch 9/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.5835 - binary_accuracy: 0.6041\n",
            "Epoch 10/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.5828 - binary_accuracy: 0.6121\n",
            "Epoch 11/20\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.5825 - binary_accuracy: 0.6073\n",
            "Epoch 12/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.5818 - binary_accuracy: 0.5998\n",
            "Epoch 13/20\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.5814 - binary_accuracy: 0.6017\n",
            "Epoch 14/20\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.5807 - binary_accuracy: 0.6116\n",
            "Epoch 15/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.5803 - binary_accuracy: 0.6072\n",
            "Epoch 16/20\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.5798 - binary_accuracy: 0.6053\n",
            "Epoch 17/20\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.5792 - binary_accuracy: 0.6105\n",
            "Epoch 18/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.5790 - binary_accuracy: 0.6068\n",
            "Epoch 19/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.5787 - binary_accuracy: 0.6131\n",
            "Epoch 20/20\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.5784 - binary_accuracy: 0.6111\n",
            "[0.5747664570808411, 0.6167696714401245]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8HGMKzAeA5l"
      },
      "source": [
        "We see a little imprvement when retrained\n",
        "Score for fold 10: loss of 0.5854668617248535; binary_accuracy of 61.59312129020691%\n",
        "\n",
        "\n",
        "and score after retrianing train set with best model - \n",
        "[loss - 0.5747664570808411, binary_accuracy - 0.6167696714401245]\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}